"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[8265],{5142:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-4-ai-integration/lesson-1-vla-architecture","title":"Lesson 1: Vision-Language-Action Architecture Overview","description":"Learning Objectives","source":"@site/docs/03-chapter-4-ai-integration/01-lesson-1-vla-architecture.md","sourceDirName":"03-chapter-4-ai-integration","slug":"/chapter-4-ai-integration/lesson-1-vla-architecture","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-1-vla-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/devhammad0/physical-ai-robotics-textbook/tree/main/docs/03-chapter-4-ai-integration/01-lesson-1-vla-architecture.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: AI Integration & Vision-Language-Action Pipelines","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/intro"},"next":{"title":"Lesson 2: Voice Input with OpenAI Whisper","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-2-voice-whisper"}}');var r=i(6070),l=i(8439);const t={},o="Lesson 1: Vision-Language-Action Architecture Overview",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"What is Vision-Language-Action (VLA)?",id:"what-is-vision-language-action-vla",level:2},{value:"The VLA Pipeline Architecture",id:"the-vla-pipeline-architecture",level:2},{value:"Key Components Explained",id:"key-components-explained",level:2},{value:"1. Speech Recognition (Whisper)",id:"1-speech-recognition-whisper",level:3},{value:"2. Vision Systems",id:"2-vision-systems",level:3},{value:"3. LLM-Based Planning",id:"3-llm-based-planning",level:3},{value:"4. Manipulation &amp; Grasping",id:"4-manipulation--grasping",level:3},{value:"5. Full-Stack Integration",id:"5-full-stack-integration",level:3},{value:"6. Safety &amp; Validation",id:"6-safety--validation",level:3},{value:"Real-World Applications",id:"real-world-applications",level:2},{value:"Home Robotics",id:"home-robotics",level:3},{value:"Manufacturing",id:"manufacturing",level:3},{value:"Logistics &amp; Warehousing",id:"logistics--warehousing",level:3},{value:"Research &amp; Development",id:"research--development",level:3},{value:"Traditional Robot Programming vs. VLA",id:"traditional-robot-programming-vs-vla",level:2},{value:"Traditional Approach",id:"traditional-approach",level:3},{value:"VLA Approach",id:"vla-approach",level:3},{value:"4-Layer Pedagogy Applied to VLA",id:"4-layer-pedagogy-applied-to-vla",level:2},{value:"Layer 1: Foundation",id:"layer-1-foundation",level:3},{value:"Layer 2: Exploration",id:"layer-2-exploration",level:3},{value:"Layer 3: Integration",id:"layer-3-integration",level:3},{value:"Layer 4: Orchestration &amp; Spec-Driven Assembly",id:"layer-4-orchestration--spec-driven-assembly",level:3},{value:"CEFR C2 Level: Complex Integration",id:"cefr-c2-level-complex-integration",level:2},{value:"What You&#39;ll Build",id:"what-youll-build",level:2},{value:"Real Hardware Considerations",id:"real-hardware-considerations",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lesson-1-vision-language-action-architecture-overview",children:"Lesson 1: Vision-Language-Action Architecture Overview"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lesson, you will:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Understand the Vision-Language-Action (VLA) pipeline conceptually (voice \u2192 perception \u2192 planning \u2192 action)"}),"\n",(0,r.jsx)(n.li,{children:"Identify where each component fits in an autonomous system architecture"}),"\n",(0,r.jsx)(n.li,{children:"Recognize real-world applications and differentiate VLA from traditional robot programming"}),"\n",(0,r.jsx)(n.li,{children:"Apply the 4-Layer pedagogy to VLA pipeline design (foundation \u2192 exploration \u2192 integration \u2192 orchestration)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"what-is-vision-language-action-vla",children:"What is Vision-Language-Action (VLA)?"}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," system is a multi-modal AI pipeline that combines:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision (Perception)"}),": Understanding the robot's environment through cameras, depth sensors, and semantic understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language (Planning)"}),": Using large language models (LLMs) to interpret human commands and decompose them into executable subtasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action (Manipulation)"}),": Executing the planned tasks through robot control (arm movement, grasping, etc.)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'Unlike traditional robot programming where humans write exact sequences of movements, VLA systems can understand natural language commands ("pick up the red cube and place it on the shelf") and autonomously figure out how to accomplish the task.'}),"\n",(0,r.jsx)(n.h2,{id:"the-vla-pipeline-architecture",children:"The VLA Pipeline Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"User Voice Command\n        \u2193\n[Speech Recognition] \u2192 Text Command\n        \u2193\n[Perception] \u2192 Detected Objects + Scene Understanding\n        \u2193\n[LLM Planning] \u2192 Task Decomposition (Subtasks)\n        \u2193\n[Motion Planning] \u2192 Collision-free Trajectories\n        \u2193\n[Execution] \u2192 Robot Actions (Move, Grasp, Place)\n        \u2193\n[Observation] \u2192 Validate Outcome & Replan if Needed\n        \u2193\nTask Complete or Replanning Loop\n"})}),"\n",(0,r.jsx)(n.h2,{id:"key-components-explained",children:"Key Components Explained"}),"\n",(0,r.jsx)(n.h3,{id:"1-speech-recognition-whisper",children:"1. Speech Recognition (Whisper)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Transcribes human voice to text"}),"\n",(0,r.jsx)(n.li,{children:"Handles background noise and accents"}),"\n",(0,r.jsx)(n.li,{children:"Returns confidence scores"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lesson 2 focus"}),": Setup and integration with ROS 2"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-vision-systems",children:"2. Vision Systems"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Object detection: "What objects are in the scene?"'}),"\n",(0,r.jsx)(n.li,{children:'Semantic segmentation: "What is each pixel?"'}),"\n",(0,r.jsx)(n.li,{children:'Depth sensing: "How far are objects?"'}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lesson 3 focus"}),": YOLO detection + semantic segmentation"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-llm-based-planning",children:"3. LLM-Based Planning"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Interprets natural language goals"}),"\n",(0,r.jsx)(n.li,{children:'Decomposes goals into subtasks: "pick up cube" \u2192 [approach object, open gripper, close gripper, lift]'}),"\n",(0,r.jsx)(n.li,{children:"Validates plans against constraints (workspace, joint limits, safety)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lesson 4 focus"}),": Prompt engineering and LLM integration"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"4-manipulation--grasping",children:"4. Manipulation & Grasping"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Grasp planning: "How should the robot hold this object?"'}),"\n",(0,r.jsx)(n.li,{children:'Motion planning: "What path should the arm take?"'}),"\n",(0,r.jsx)(n.li,{children:'Force control: "How hard to grip without crushing?"'}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lesson 5 focus"}),": Grasp planning + motion planning algorithms"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"5-full-stack-integration",children:"5. Full-Stack Integration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Orchestrates all components into a unified system"}),"\n",(0,r.jsx)(n.li,{children:"Handles asynchronous communication and failures"}),"\n",(0,r.jsx)(n.li,{children:"Implements closed-loop execution (plan \u2192 execute \u2192 observe \u2192 replan)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lesson 6 focus"}),": Orchestration architecture"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"6-safety--validation",children:"6. Safety & Validation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Kill switch: Emergency stop capability"}),"\n",(0,r.jsx)(n.li,{children:"Workspace boundaries: Prevent motion outside safe region"}),"\n",(0,r.jsx)(n.li,{children:"Force limits: Prevent over-gripping and collisions"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lesson 7 focus"}),": Safety protocols and deployment"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,r.jsx)(n.h3,{id:"home-robotics",children:"Home Robotics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Robot tidies living room: "Clean up the table" \u2192 detects items \u2192 grasps and moves to proper locations'}),"\n",(0,r.jsx)(n.li,{children:'Elderly care: "Bring me the remote" \u2192 searches home \u2192 retrieves and delivers object'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"manufacturing",children:"Manufacturing"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Assembly automation: "Assemble part A to part B" \u2192 perceives parts \u2192 positions and fastens them'}),"\n",(0,r.jsx)(n.li,{children:'Quality inspection: "Check for defects" \u2192 captures images \u2192 analyzes for flaws'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"logistics--warehousing",children:"Logistics & Warehousing"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Order fulfillment: "Pack items for order #12345" \u2192 perceives items \u2192 grasps and arranges in box'}),"\n",(0,r.jsx)(n.li,{children:'Inventory management: "Stock shelves" \u2192 scans barcodes \u2192 places items on correct shelves'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"research--development",children:"Research & Development"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Lab automation: "Run PCR protocol" \u2192 perceives samples \u2192 manipulates equipment and transfers samples'}),"\n",(0,r.jsx)(n.li,{children:"Scientific discovery: Interactive experiments where robots can ask questions and adapt"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"traditional-robot-programming-vs-vla",children:"Traditional Robot Programming vs. VLA"}),"\n",(0,r.jsx)(n.h3,{id:"traditional-approach",children:"Traditional Approach"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Hardcoded sequence\nrobot.move_to([0.5, 0.3, 0.8])  # Approach cube\nrobot.open_gripper()\nrobot.move_to([0.5, 0.3, 0.6])  # Close on cube\nrobot.close_gripper()\nrobot.move_to([0.5, 0.7, 0.8])  # Move to shelf\nrobot.open_gripper()  # Release\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Must code every variation (red cube, blue sphere, etc.)"}),"\n",(0,r.jsx)(n.li,{children:"Breaks if environment changes (object moves)"}),"\n",(0,r.jsx)(n.li,{children:"No understanding of task goal"}),"\n",(0,r.jsx)(n.li,{children:"Requires rewriting for each new task"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"vla-approach",children:"VLA Approach"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'User: "Pick up the red cube and place it on the shelf"\nSystem: [Automatically handles object detection, grasp planning, path planning, error recovery]\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understands natural language goals"}),"\n",(0,r.jsx)(n.li,{children:"Adapts to environment variations"}),"\n",(0,r.jsx)(n.li,{children:"Recovers from failures (replans if grasp fails)"}),"\n",(0,r.jsx)(n.li,{children:"Same system handles multiple tasks"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"4-layer-pedagogy-applied-to-vla",children:"4-Layer Pedagogy Applied to VLA"}),"\n",(0,r.jsxs)(n.p,{children:["This course uses a ",(0,r.jsx)(n.strong,{children:"4-layer progression"})," to build understanding:"]}),"\n",(0,r.jsx)(n.h3,{id:"layer-1-foundation",children:"Layer 1: Foundation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"What"}),": Understanding individual components"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lessons 1-3"}),": Speech recognition, vision, planning basics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Each component works independently"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"layer-2-exploration",children:"Layer 2: Exploration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"What"}),": Pairwise integration and testing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lessons 2-3"}),": Voice + perception together"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Components communicate correctly"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"layer-3-integration",children:"Layer 3: Integration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"What"}),": Full-stack system assembly"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lesson 6"}),": All components working together"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": End-to-end autonomous system"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"layer-4-orchestration--spec-driven-assembly",children:"Layer 4: Orchestration & Spec-Driven Assembly"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"What"}),": Reliable, deployable systems with safety"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lesson 7"}),": Safety, error handling, deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Production-ready autonomous system"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"cefr-c2-level-complex-integration",children:"CEFR C2 Level: Complex Integration"}),"\n",(0,r.jsxs)(n.p,{children:["Chapter 4 targets ",(0,r.jsx)(n.strong,{children:"CEFR C2"})," (highest proficiency level), meaning:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Complex concepts"}),": Multi-modal AI, real-time control, fault tolerance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integration mastery"}),": Understand how components interact at system level"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-world tradeoffs"}),": Performance vs. accuracy, latency vs. quality, cost vs. capability"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deployment readiness"}),": Safety, testing, monitoring, scaling"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This is not a beginner's chapter\u2014it assumes solid ROS 2, simulation, and navigation knowledge from Chapters 1-3."}),"\n",(0,r.jsx)(n.h2,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,r.jsx)(n.p,{children:"By the end of Chapter 4, you will have implemented:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   VLA Autonomous System in Isaac Sim    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Lesson 1: Architecture (this lesson)   \u2502\n\u2502  Lesson 2: Voice Input (Whisper)        \u2502\n\u2502  Lesson 3: Vision (Detection + Seg)     \u2502\n\u2502  Lesson 4: Planning (LLM Decomposition) \u2502\n\u2502  Lesson 5: Manipulation (Grasp + Motion)\u2502\n\u2502  Lesson 6: Integration (Orchestrator)   \u2502\n\u2502  Lesson 7: Safety (Kill Switch, Bounds) \u2502\n\u2502  Lesson 8: Capstone (Your Own Project)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.p,{children:"A complete system that can:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Listen to voice commands"}),"\n",(0,r.jsx)(n.li,{children:"Perceive the environment"}),"\n",(0,r.jsx)(n.li,{children:"Plan multi-step tasks"}),"\n",(0,r.jsx)(n.li,{children:"Manipulate objects"}),"\n",(0,r.jsx)(n.li,{children:"Recover from failures"}),"\n",(0,r.jsx)(n.li,{children:"Operate safely"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"real-hardware-considerations",children:"Real Hardware Considerations"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Simulation vs. Real Robot:"})}),"\n",(0,r.jsxs)(n.p,{children:["In this chapter, all development happens in ",(0,r.jsx)(n.strong,{children:"Isaac Sim"})," (simulation). However, deployment to real hardware requires understanding key differences:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Quality"}),": Real cameras have noise, limited resolution, occlusion; simulation has perfect sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Physics Accuracy"}),": Simulated physics is perfect; real materials have friction, deformation, variability"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency"}),": Simulated communication is instantaneous; real networks have delays"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety"}),": Simulation can't damage equipment; real robots can injure people or damage objects"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'Each subsequent lesson includes a "Real Hardware" section explaining what changes when moving from simulation to physical robots.'}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VLA pipelines integrate multiple AI/ML systems"})," (speech, vision, planning, control)"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Traditional programming is replaced by goal-driven autonomous behavior"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Each component can be developed and tested independently"})," (4-layer approach)"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Real-world deployment requires careful consideration of safety, latency, and robustness"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"This course teaches simulation-first methodology"})," for safe, validated development"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.strong,{children:"Lesson 2"}),", you'll implement your first VLA component: ",(0,r.jsx)(n.strong,{children:"speech recognition with OpenAI Whisper"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"You'll set up a ROS 2 node that:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Captures audio from your microphone"}),"\n",(0,r.jsx)(n.li,{children:"Sends it to Whisper for transcription"}),"\n",(0,r.jsx)(n.li,{children:"Publishes the recognized text to ROS 2 topics"}),"\n",(0,r.jsx)(n.li,{children:"Handles errors gracefully"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"See you in Lesson 2! \ud83c\udfa4\ud83e\udd16"})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8439:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(758);const r={},l=s.createContext(r);function t(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);