"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[888],{1257:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"chapter-4-vla/intro","title":"Chapter 4: Vision-Language-Action Models","description":"Vision-Language-Action (VLA) models represent the frontier of embodied AI, enabling robots to understand visual scenes and natural language instructions to take physical actions.","source":"@site/docs/chapter-4-vla/intro.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/intro","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-vla/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/devhammad0/physical-ai-robotics-textbook/tree/main/docs/chapter-4-vla/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 5: NVIDIA Isaac Sim","permalink":"/physical-ai-robotics-textbook/docs/chapter-3-isaac/lesson-05"},"next":{"title":"Lesson 1: Vision-Language-Action Models","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-vla/lesson-01"}}');var i=s(6070),t=s(8439);const r={sidebar_position:1},l="Chapter 4: Vision-Language-Action Models",a={},d=[{value:"Module Overview",id:"module-overview",level:2},{value:"What You&#39;ll Learn",id:"what-youll-learn",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Tools and Setup",id:"tools-and-setup",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-4-vision-language-action-models",children:"Chapter 4: Vision-Language-Action Models"})}),"\n",(0,i.jsx)(n.p,{children:"Vision-Language-Action (VLA) models represent the frontier of embodied AI, enabling robots to understand visual scenes and natural language instructions to take physical actions."}),"\n",(0,i.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Duration"}),": 5 lessons"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Prerequisites"}),": Modules 1-3 (ROS 2, Gazebo, Isaac Sim)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Skills"}),": Multimodal AI, foundation models, embodied reasoning"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multimodal Architectures"}),": Vision transformers and language models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Foundation Models for Robotics"}),": Pre-trained models (OpenAI CLIP, others)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Prediction"}),": From vision-language to robot control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Training and Fine-tuning"}),": Adapting VLA models to robots"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-World Deployment"}),": Deploying VLA models on robots"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Evaluation and Benchmarking"}),": Assessing robot performance"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand VLA model architectures"}),"\n",(0,i.jsx)(n.li,{children:"Use pre-trained vision-language models"}),"\n",(0,i.jsx)(n.li,{children:"Fine-tune models for robotic tasks"}),"\n",(0,i.jsx)(n.li,{children:"Design end-to-end learning pipelines"}),"\n",(0,i.jsx)(n.li,{children:"Deploy VLA systems on physical robots"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate and improve robot performance"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"tools-and-setup",children:"Tools and Setup"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Frameworks"}),": PyTorch, Hugging Face Transformers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Models"}),": CLIP, GPT-4V, Gato, and others"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hardware"}),": NVIDIA GPU (RTX 3090+, A100)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration"}),": ROS 2, Isaac Sim, real robots"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"Start with Lesson 1: Vision-Language Model Fundamentals \u2192"})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8439:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var o=s(758);const i={},t=o.createContext(i);function r(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);