"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[5752],{8439:(n,e,r)=>{r.d(e,{R:()=>l,x:()=>a});var s=r(758);const i={},t=s.createContext(i);function l(n){const e=s.useContext(t);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:l(n.components),s.createElement(t.Provider,{value:e},n.children)}},8903:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>o,contentTitle:()=>a,default:()=>p,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-4-ai-integration/lesson-4-llm-planning","title":"Lesson 4: LLM-Driven Planning & Task Decomposition","description":"Learning Objectives","source":"@site/docs/03-chapter-4-ai-integration/04-lesson-4-llm-planning.md","sourceDirName":"03-chapter-4-ai-integration","slug":"/chapter-4-ai-integration/lesson-4-llm-planning","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-4-llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/devhammad0/physical-ai-robotics-textbook/tree/main/docs/03-chapter-4-ai-integration/04-lesson-4-llm-planning.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3: Vision Systems for Robotics","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-3-vision-systems"},"next":{"title":"Lesson 5: Manipulation, Grasping & Motion Planning","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-5-manipulation-grasping"}}');var i=r(6070),t=r(8439);const l={},a="Lesson 4: LLM-Driven Planning & Task Decomposition",o={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"What is LLM-Based Planning?",id:"what-is-llm-based-planning",level:2},{value:"Why LLMs for Planning?",id:"why-llms-for-planning",level:3},{value:"LLM Limitations (Why We Still Need Validation)",id:"llm-limitations-why-we-still-need-validation",level:3},{value:"LLM Planning Architecture",id:"llm-planning-architecture",level:2},{value:"LLM Selection: OpenAI vs. Ollama",id:"llm-selection-openai-vs-ollama",level:2},{value:"Option 1: OpenAI GPT-4 (Recommended for accuracy)",id:"option-1-openai-gpt-4-recommended-for-accuracy",level:3},{value:"Option 2: Ollama (Open-source, Local)",id:"option-2-ollama-open-source-local",level:3},{value:"Prompt Engineering for Robot Planning",id:"prompt-engineering-for-robot-planning",level:2},{value:"ROS 2 LLM Planner Node",id:"ros-2-llm-planner-node",level:2},{value:"Prompt Templates for Different Tasks",id:"prompt-templates-for-different-tasks",level:2},{value:"Template 1: Object Manipulation",id:"template-1-object-manipulation",level:3},{value:"Template 2: Scene Exploration",id:"template-2-scene-exploration",level:3},{value:"Template 3: Collaborative Tasks",id:"template-3-collaborative-tasks",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise L4-1: Prompt Engineering",id:"exercise-l4-1-prompt-engineering",level:3},{value:"Exercise L4-2: Implement Planning Node",id:"exercise-l4-2-implement-planning-node",level:3},{value:"Real Hardware Considerations",id:"real-hardware-considerations",level:2},{value:"Differences from Simulation",id:"differences-from-simulation",level:3},{value:"Real Hardware Strategies",id:"real-hardware-strategies",level:3},{value:"Simulation-to-Reality Transfer",id:"simulation-to-reality-transfer",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"lesson-4-llm-driven-planning--task-decomposition",children:"Lesson 4: LLM-Driven Planning & Task Decomposition"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this lesson, you will:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Understand how large language models (LLMs) decompose high-level goals into executable robot tasks"}),"\n",(0,i.jsx)(e.li,{children:"Use prompt engineering to guide LLM planning (constraints, format, examples)"}),"\n",(0,i.jsx)(e.li,{children:"Implement a ROS 2 planning node that integrates GPT-4 or Ollama"}),"\n",(0,i.jsx)(e.li,{children:"Validate plans against safety constraints (workspace, joint limits, gripper feasibility)"}),"\n",(0,i.jsx)(e.li,{children:"Handle LLM errors gracefully and implement fallback strategies"}),"\n",(0,i.jsx)(e.li,{children:"Understand real hardware planning challenges (uncertainty, replanning, learning from failures)"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"what-is-llm-based-planning",children:"What is LLM-Based Planning?"}),"\n",(0,i.jsx)(e.p,{children:"Traditional robot programming requires humans to specify every movement:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Traditional: Hard-coded sequence\r\nrobot.move_to([0.5, 0.3, 0.8])  # Approach cube\r\nrobot.open_gripper()\r\nrobot.move_to([0.5, 0.3, 0.6])  # Close on cube\r\nrobot.close_gripper()\n"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"LLM-based planning"})," lets the robot understand natural language and figure out the steps:"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:'User: "Pick up the red cube and place it on the shelf"\r\n     \u2193\r\nLLM Planner\r\n     \u2193\r\nOutput: [\r\n  "detect red cube in scene",\r\n  "approach cube center at pixel location",\r\n  "open gripper",\r\n  "move arm down to object",\r\n  "close gripper with appropriate force",\r\n  "lift object to 0.8m height",\r\n  "move to shelf location",\r\n  "place on shelf",\r\n  "open gripper"\r\n]\r\n     \u2193\r\nExecutor runs each step\n'})}),"\n",(0,i.jsx)(e.h3,{id:"why-llms-for-planning",children:"Why LLMs for Planning?"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Flexibility"}),": Same system handles diverse tasks without code changes"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Interpretability"}),": Can explain reasoning (why approach from this angle?)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Generalization"}),": Training on human reasoning helps handle novel situations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Error Recovery"}),": Can replan when execution fails"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Constraint Awareness"}),": Can incorporate safety rules into planning"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"llm-limitations-why-we-still-need-validation",children:"LLM Limitations (Why We Still Need Validation)"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Hallucination"}),": LLM might suggest physically impossible actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Real-time blindness"}),": LLM doesn't see actual robot state, only initial perception"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cost"}),": API calls to GPT-4 are expensive ($0.03-0.10 per request)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Latency"}),": API calls take 2-3 seconds (vs. <100ms for traditional planning)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Determinism"}),": Same input may generate different outputs"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"llm-planning-architecture",children:"LLM Planning Architecture"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Voice Command: "Pick up red cube"                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                 \u2502\r\n                 \u2193\r\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n        \u2502  Input Validation  \u2502\r\n        \u2502 \u2022 Transcription OK?\u2502\r\n        \u2502 \u2022 Confidence \u22650.7? \u2502\r\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                 \u2502\r\n                 \u2193\r\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n        \u2502  Perception Integration    \u2502\r\n        \u2502 \u2022 Detected objects: [...]  \u2502\r\n        \u2502 \u2022 Scene segmentation       \u2502\r\n        \u2502 \u2022 Estimated positions      \u2502\r\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                 \u2502\r\n                 \u2193\r\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n    \u2502  Build LLM Prompt                  \u2502\r\n    \u2502 \u2022 System prompt (robot capabilities)\u2502\r\n    \u2502 \u2022 Task description                 \u2502\r\n    \u2502 \u2022 Detected objects                 \u2502\r\n    \u2502 \u2022 Output format (JSON)             \u2502\r\n    \u2502 \u2022 Constraints (workspace, limits)  \u2502\r\n    \u2502 \u2022 Few-shot examples                \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n             \u2502\r\n             \u2193\r\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n    \u2502  LLM Planning Service           \u2502\r\n    \u2502  (GPT-4 or Ollama)              \u2502\r\n    \u2502  \u2022 Send prompt                  \u2502\r\n    \u2502  \u2022 Parse JSON response          \u2502\r\n    \u2502  \u2022 Extract action sequence      \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n             \u2502\r\n             \u2193\r\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n    \u2502  Plan Validation                \u2502\r\n    \u2502 \u2022 Workspace constraints OK?     \u2502\r\n    \u2502 \u2022 Joint limits OK?              \u2502\r\n    \u2502 \u2022 Gripper feasible?             \u2502\r\n    \u2502 \u2022 Physics plausible?            \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n             \u2502 (Pass)\r\n             \u2193\r\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n    \u2502  Task Plan Output               \u2502\r\n    \u2502 TaskPlan {                      \u2502\r\n    \u2502  task_id: "uuid",              \u2502\r\n    \u2502  subtasks: [...],              \u2502\r\n    \u2502  is_feasible: true             \u2502\r\n    \u2502 }                               \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,i.jsx)(e.h2,{id:"llm-selection-openai-vs-ollama",children:"LLM Selection: OpenAI vs. Ollama"}),"\n",(0,i.jsx)(e.h3,{id:"option-1-openai-gpt-4-recommended-for-accuracy",children:"Option 1: OpenAI GPT-4 (Recommended for accuracy)"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Advantages"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Highest quality reasoning and instruction following"}),"\n",(0,i.jsx)(e.li,{children:"Handles complex decomposition"}),"\n",(0,i.jsx)(e.li,{children:"~90%+ success rate on robot tasks"}),"\n",(0,i.jsx)(e.li,{children:"Automatically improves as OpenAI updates model"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Disadvantages"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Requires API key + internet connection"}),"\n",(0,i.jsx)(e.li,{children:"Costs $0.03-0.10 per request (~$3-10 per 100 tasks)"}),"\n",(0,i.jsx)(e.li,{children:"2-3 second latency (slower than local)"}),"\n",(0,i.jsx)(e.li,{children:"Privacy: requests sent to OpenAI servers"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Installation"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:'pip install openai\r\nexport OPENAI_API_KEY="sk-..."  # Get from https://platform.openai.com/api-keys\n'})}),"\n",(0,i.jsx)(e.h3,{id:"option-2-ollama-open-source-local",children:"Option 2: Ollama (Open-source, Local)"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Advantages"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Free, runs locally on your machine"}),"\n",(0,i.jsx)(e.li,{children:"No API keys required"}),"\n",(0,i.jsx)(e.li,{children:"Private: data never leaves your computer"}),"\n",(0,i.jsx)(e.li,{children:"No per-request cost"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Disadvantages"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Lower quality reasoning than GPT-4"}),"\n",(0,i.jsx)(e.li,{children:"Requires 4-8GB VRAM and modern GPU"}),"\n",(0,i.jsx)(e.li,{children:"Slower inference (5-10 seconds per plan)"}),"\n",(0,i.jsx)(e.li,{children:"Need to download large model (~7GB)"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Installation"}),":"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Download Ollama from https://ollama.ai\r\nollama pull mistral  # ~5GB, good reasoning model\r\n# or: ollama pull llama2  # Alternative\n"})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"For this course"}),": We recommend ",(0,i.jsx)(e.strong,{children:"GPT-4"})," for better reliability, but Ollama works fine for learning."]}),"\n",(0,i.jsx)(e.h2,{id:"prompt-engineering-for-robot-planning",children:"Prompt Engineering for Robot Planning"}),"\n",(0,i.jsxs)(e.p,{children:["The key to good LLM planning is a ",(0,i.jsx)(e.strong,{children:"well-structured prompt"}),". Here's the architecture:"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:'[System Prompt]\r\nYou are a robot planning system. Your job is to decompose high-level\r\ncommands into executable robot actions. You must respect constraints.\r\n\r\n[Robot Capabilities]\r\n- Can detect objects via camera (YOLO-v8)\r\n- Can move arm to any workspace position\r\n- Can open/close gripper with force control\r\n- Cannot reach outside workspace [-0.5, -0.5, 0.3] to [1.0, 0.5, 2.0]\r\n- Maximum gripper force: 100N\r\n\r\n[Current Perception]\r\n- Detected objects: red cube at [0.5, 0.3, 0.1], blue sphere at [0.3, 0.2, 0.05]\r\n- Segmentation: table (0.1-0.8m height), shelf (1.2-1.8m height)\r\n- Estimated 3D positions: [x, y, z] in meters\r\n\r\n[Task]\r\nUser said: "Pick up the red cube and place it on the shelf"\r\n\r\n[Output Format]\r\nReturn valid JSON only, no other text:\r\n{\r\n  "task_id": "uuid",\r\n  "task_description": "...",\r\n  "subtasks": ["step 1", "step 2", ...],\r\n  "constraints_respected": true,\r\n  "reasoning": "Why these steps?"\r\n}\r\n\r\n[Examples]\r\nExample 1: "Find the red cube"\r\n{\r\n  "subtasks": ["scan room with camera", "detect red objects",\r\n               "locate red cube in 3D space", "report position"],\r\n  "constraints_respected": true\r\n}\r\n\r\n[Constraints]\r\n- Each subtask must be < 2 sentences\r\n- No reaching outside workspace\r\n- Gripper force must be \u2264100N\r\n- Cannot manipulate objects simultaneously\n'})}),"\n",(0,i.jsx)(e.h2,{id:"ros-2-llm-planner-node",children:"ROS 2 LLM Planner Node"}),"\n",(0,i.jsxs)(e.p,{children:["File: ",(0,i.jsx)(e.code,{children:"chapter4_planning/src/llm_planner_node.py"})]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport json\r\nimport os\r\nfrom datetime import datetime\r\nimport uuid\r\n\r\nclass LLMPlannerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'llm_planner_node\')\r\n\r\n        # Load configuration\r\n        self.use_openai = self.declare_parameter(\r\n            \'use_openai\', True\r\n        ).value\r\n\r\n        if self.use_openai:\r\n            try:\r\n                import openai\r\n                openai.api_key = os.getenv(\'OPENAI_API_KEY\')\r\n                self.openai_client = openai\r\n                self.get_logger().info(\'Using OpenAI GPT-4 for planning\')\r\n            except ImportError:\r\n                self.get_logger().error(\'OpenAI library not installed. Install with: pip install openai\')\r\n                raise\r\n        else:\r\n            try:\r\n                import ollama\r\n                self.ollama_client = ollama\r\n                self.get_logger().info(\'Using Ollama (local) for planning\')\r\n            except ImportError:\r\n                self.get_logger().error(\'Ollama library not installed\')\r\n                raise\r\n\r\n        # Subscribe to voice commands and detections\r\n        self.voice_sub = self.create_subscription(\r\n            String,\r\n            \'/robot/voice_command\',\r\n            self.voice_callback,\r\n            10\r\n        )\r\n\r\n        self.detections_sub = self.create_subscription(\r\n            String,\r\n            \'/robot/detections\',\r\n            self.detections_callback,\r\n            10\r\n        )\r\n\r\n        # Publish plans\r\n        self.plan_pub = self.create_publisher(\r\n            String,\r\n            \'/robot/task_plan\',\r\n            10\r\n        )\r\n\r\n        # Store last detection data\r\n        self.last_detections = None\r\n\r\n        # Configuration\r\n        self.workspace_min = [-0.5, -0.5, 0.3]\r\n        self.workspace_max = [1.0, 0.5, 2.0]\r\n        self.max_gripper_force = 100.0\r\n\r\n        self.get_logger().info(\'LLM Planner Node initialized\')\r\n\r\n    def voice_callback(self, msg):\r\n        """Process voice command and generate plan"""\r\n        command = msg.data\r\n        self.get_logger().info(f\'Planning for command: "{command}"\')\r\n\r\n        # Generate plan\r\n        plan = self.generate_plan(command, self.last_detections)\r\n\r\n        if plan:\r\n            # Publish plan\r\n            plan_msg = String()\r\n            plan_msg.data = json.dumps(plan)\r\n            self.plan_pub.publish(plan_msg)\r\n            self.get_logger().info(f\'Published plan with {len(plan["subtasks"])} subtasks\')\r\n        else:\r\n            self.get_logger().error(\'Failed to generate plan\')\r\n\r\n    def detections_callback(self, msg):\r\n        """Store latest detection data for planning context"""\r\n        try:\r\n            self.last_detections = json.loads(msg.data)\r\n        except json.JSONDecodeError:\r\n            self.get_logger().warn(\'Failed to parse detections JSON\')\r\n\r\n    def generate_plan(self, command: str, detections=None) -> dict:\r\n        """Generate task plan using LLM"""\r\n        try:\r\n            # Build detection context\r\n            detection_str = self._format_detections(detections)\r\n\r\n            # Build prompt\r\n            system_prompt = self._get_system_prompt()\r\n            user_prompt = f"""\r\nTask: {command}\r\n\r\nCurrent Perception:\r\n{detection_str}\r\n\r\nWorkspace bounds: {self.workspace_min} to {self.workspace_max}\r\nMax gripper force: {self.max_gripper_force}N\r\n\r\nGenerate a JSON plan with subtasks.\r\n"""\r\n\r\n            # Call LLM\r\n            if self.use_openai:\r\n                response = self.openai_client.ChatCompletion.create(\r\n                    model="gpt-4",\r\n                    messages=[\r\n                        {"role": "system", "content": system_prompt},\r\n                        {"role": "user", "content": user_prompt}\r\n                    ],\r\n                    temperature=0.3,\r\n                    max_tokens=500\r\n                )\r\n                response_text = response[\'choices\'][0][\'message\'][\'content\']\r\n            else:\r\n                response = self.ollama_client.generate(\r\n                    model=\'mistral\',\r\n                    prompt=f"{system_prompt}\\n{user_prompt}",\r\n                    stream=False\r\n                )\r\n                response_text = response[\'response\']\r\n\r\n            # Parse response\r\n            plan = self._parse_plan_response(response_text, command)\r\n\r\n            # Validate plan\r\n            is_valid, reason = self._validate_plan(plan)\r\n            plan[\'is_feasible\'] = is_valid\r\n            plan[\'feasibility_reason\'] = reason if not is_valid else "OK"\r\n\r\n            return plan\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f\'LLM planning failed: {e}\')\r\n            return None\r\n\r\n    def _get_system_prompt(self) -> str:\r\n        """Get system prompt for LLM"""\r\n        return """You are a robot planning system. Your job is to decompose\r\nhigh-level commands into executable robot actions.\r\n\r\nRobot Capabilities:\r\n- Can detect objects via YOLO-v8 camera\r\n- Can move arm to any position in workspace\r\n- Can grasp and place objects\r\n- Can move at safe speeds to avoid collisions\r\n\r\nOutput exactly this JSON format, no other text:\r\n{\r\n  "task_id": "uuid-string",\r\n  "task_description": "Human-readable task",\r\n  "subtasks": ["step 1", "step 2", ...],\r\n  "estimated_duration": 15.5,\r\n  "reasoning": "Why these steps?"\r\n}\r\n\r\nKeep each subtask simple (<2 sentences).\r\nRespect workspace boundaries and force limits.\r\n"""\r\n\r\n    def _format_detections(self, detections) -> str:\r\n        """Format detection data for LLM context"""\r\n        if not detections:\r\n            return "No objects detected"\r\n\r\n        try:\r\n            lines = ["Detected objects:"]\r\n            for det in detections.get(\'detections\', []):\r\n                lines.append(f"  - {det[\'class\']}: confidence {det[\'confidence\']:.2f}, "\r\n                           f"at [{det[\'bbox\'][\'x1\']}, {det[\'bbox\'][\'y1\']}]")\r\n            return "\\n".join(lines)\r\n        except (KeyError, TypeError):\r\n            return "Could not parse detections"\r\n\r\n    def _parse_plan_response(self, response_text: str, command: str) -> dict:\r\n        """Parse LLM response into plan structure"""\r\n        try:\r\n            # Extract JSON from response (LLM might add extra text)\r\n            json_start = response_text.find(\'{\')\r\n            json_end = response_text.rfind(\'}\') + 1\r\n            if json_start >= 0 and json_end > json_start:\r\n                json_text = response_text[json_start:json_end]\r\n                plan = json.loads(json_text)\r\n            else:\r\n                raise ValueError("No JSON found in response")\r\n\r\n            # Ensure required fields\r\n            if \'task_id\' not in plan:\r\n                plan[\'task_id\'] = str(uuid.uuid4())\r\n            if \'task_description\' not in plan:\r\n                plan[\'task_description\'] = command\r\n            if \'subtasks\' not in plan:\r\n                plan[\'subtasks\'] = []\r\n            if \'estimated_duration\' not in plan:\r\n                plan[\'estimated_duration\'] = 30.0\r\n\r\n            return plan\r\n\r\n        except (json.JSONDecodeError, ValueError) as e:\r\n            self.get_logger().warn(f\'Failed to parse plan: {e}, using default\')\r\n            return {\r\n                \'task_id\': str(uuid.uuid4()),\r\n                \'task_description\': command,\r\n                \'subtasks\': [\'execute user command\'],\r\n                \'estimated_duration\': 30.0,\r\n                \'reasoning\': \'Failed to parse LLM response\'\r\n            }\r\n\r\n    def _validate_plan(self, plan: dict) -> tuple:\r\n        """Validate plan against constraints"""\r\n        # Check for reasonable number of subtasks\r\n        if len(plan.get(\'subtasks\', [])) > 20:\r\n            return False, "Too many subtasks (>20)"\r\n\r\n        # Check estimated duration\r\n        duration = plan.get(\'estimated_duration\', 0)\r\n        if duration > 120:\r\n            return False, "Estimated duration too long (>120s)"\r\n        if duration < 1:\r\n            return False, "Estimated duration too short (<1s)"\r\n\r\n        # Check for obviously unsafe subtasks\r\n        unsafe_keywords = [\'explode\', \'destroy\', \'hurt\', \'dangerous\', \'unsafe\']\r\n        for subtask in plan.get(\'subtasks\', []):\r\n            if any(keyword in subtask.lower() for keyword in unsafe_keywords):\r\n                return False, f"Unsafe subtask detected: {subtask}"\r\n\r\n        return True, "OK"\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    planner_node = LLMPlannerNode()\r\n    rclpy.spin(planner_node)\r\n    rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"prompt-templates-for-different-tasks",children:"Prompt Templates for Different Tasks"}),"\n",(0,i.jsx)(e.h3,{id:"template-1-object-manipulation",children:"Template 1: Object Manipulation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'OBJECT_MANIPULATION_PROMPT = """\r\nTask: Pick up the {color} {object_type} and place it on the {location}\r\n\r\nCurrent scene:\r\n- {objects_list}\r\n\r\nRobot workspace: {workspace}\r\nAvailable actions:\r\n  - detect_object(color, type) -> 3D position\r\n  - move_arm(target_position)\r\n  - open_gripper()\r\n  - close_gripper(force)\r\n  - check_collision()\r\n\r\nPlan steps ensuring:\r\n1. Detect target object\r\n2. Plan collision-free path\r\n3. Approach from stable angle\r\n4. Grasp with appropriate force\r\n5. Lift safely\r\n6. Move to destination\r\n7. Place object\r\n8. Verify success\r\n"""\n'})}),"\n",(0,i.jsx)(e.h3,{id:"template-2-scene-exploration",children:"Template 2: Scene Exploration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'SCENE_EXPLORATION_PROMPT = """\r\nTask: Explore the environment and categorize all objects\r\n\r\nAvailable cameras: RGB + depth\r\nAvailable sensors: gripper force/torque\r\n\r\nGenerate plan to:\r\n1. Scan environment from multiple angles\r\n2. Build 3D map of scene\r\n3. Classify objects by type\r\n4. Estimate object properties (size, material)\r\n5. Identify stable surfaces\r\n6. Report findings\r\n"""\n'})}),"\n",(0,i.jsx)(e.h3,{id:"template-3-collaborative-tasks",children:"Template 3: Collaborative Tasks"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'COLLABORATIVE_PROMPT = """\r\nTask: {user_instruction}\r\n\r\nAvailable resources:\r\n- Robot 1: 7-DOF arm, parallel gripper\r\n- Robot 2: Mobile base + arm\r\n\r\nCoordinate actions to:\r\n1. Distribute subtasks between robots\r\n2. Avoid collisions\r\n3. Synchronize timing\r\n4. Report completion status\r\n"""\n'})}),"\n",(0,i.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(e.h3,{id:"exercise-l4-1-prompt-engineering",children:"Exercise L4-1: Prompt Engineering"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Objective"}),": Understand how prompts affect plan quality"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:['Write 3 different prompts for "pick up the cube"',"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Prompt A: Minimal (just the task)"}),"\n",(0,i.jsx)(e.li,{children:"Prompt B: With constraints (workspace, force limits)"}),"\n",(0,i.jsx)(e.li,{children:"Prompt C: With few-shot examples (show good/bad plans)"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.li,{children:"Test each prompt 3 times"}),"\n",(0,i.jsx)(e.li,{children:"Compare output quality (specificity, safety, feasibility)"}),"\n",(0,i.jsx)(e.li,{children:"Which prompt produces best results?"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Success Criteria"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u2713 Can articulate how constraints affect planning"}),"\n",(0,i.jsx)(e.li,{children:"\u2713 Understand prompt structure and components"}),"\n",(0,i.jsx)(e.li,{children:"\u2713 Recognize hallucinations and how to prevent them"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"exercise-l4-2-implement-planning-node",children:"Exercise L4-2: Implement Planning Node"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Objective"}),": Create LLM planner that integrates with voice and perception"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["Implement ",(0,i.jsx)(e.code,{children:"llm_planner_node.py"})," from this lesson"]}),"\n",(0,i.jsxs)(e.li,{children:["Set OpenAI API key: ",(0,i.jsx)(e.code,{children:'export OPENAI_API_KEY="sk-..."'})]}),"\n",(0,i.jsx)(e.li,{children:"Launch Whisper + Vision nodes (Lessons 2-3)"}),"\n",(0,i.jsxs)(e.li,{children:["Launch planner node: ",(0,i.jsx)(e.code,{children:"ros2 run chapter4_planning llm_planner_node"})]}),"\n",(0,i.jsx)(e.li,{children:'Say voice command: "Pick up the red cube"'}),"\n",(0,i.jsxs)(e.li,{children:["Verify plan published to ",(0,i.jsx)(e.code,{children:"/robot/task_plan"})]}),"\n",(0,i.jsx)(e.li,{children:"Check plan JSON for reasonable subtasks"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Success Criteria"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"\u2713 Node launches without errors"}),"\n",(0,i.jsx)(e.li,{children:"\u2713 Voice command parsed correctly"}),"\n",(0,i.jsx)(e.li,{children:"\u2713 Plan generated within 3 seconds"}),"\n",(0,i.jsx)(e.li,{children:"\u2713 Plan is valid JSON with subtasks"}),"\n",(0,i.jsx)(e.li,{children:"\u2713 Constraints validated (workspace, force)"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"real-hardware-considerations",children:"Real Hardware Considerations"}),"\n",(0,i.jsx)(e.h3,{id:"differences-from-simulation",children:"Differences from Simulation"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Simulation (Isaac Sim)"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Perfect knowledge of object positions (no estimation error)"}),"\n",(0,i.jsx)(e.li,{children:"Deterministic physics (same action \u2192 same result every time)"}),"\n",(0,i.jsx)(e.li,{children:"No latency or communication delays"}),"\n",(0,i.jsx)(e.li,{children:"LLM planning overhead acceptable (3s latency invisible in sim)"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Real Hardware"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Object positions uncertain (camera calibration error ~5cm)"}),"\n",(0,i.jsx)(e.li,{children:"Physics uncertain (friction, material properties unknown)"}),"\n",(0,i.jsx)(e.li,{children:"Network latency and communication failures possible"}),"\n",(0,i.jsx)(e.li,{children:"Humans expect near-instant response (long planning = frustrating)"}),"\n",(0,i.jsx)(e.li,{children:"Object appearance varies (lighting, wear, dirt)"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"real-hardware-strategies",children:"Real Hardware Strategies"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Faster LLMs"}),": Use local Ollama instead of GPT-4 API (saves 2-3s latency)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Caching"}),": Store common plans, avoid re-planning identical tasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Incremental planning"}),": Plan just the next step, not entire task"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Learning from failures"}),": Log failed plans, use to improve prompts"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human oversight"}),": Have humans approve critical plans before execution"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Uncertainty quantification"}),": Track confidence in each subtask"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"simulation-to-reality-transfer",children:"Simulation-to-Reality Transfer"}),"\n",(0,i.jsxs)(e.p,{children:["The biggest challenge: ",(0,i.jsx)(e.strong,{children:"LLM planning works differently on real robots"})]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"In simulation"}),': "Pick up cube at [0.5, 0.3, 0.1]" \u2192 always works if coordinates valid']}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"On real robot"}),': "Pick up cube" \u2192 might fail if camera calibration off, gripper slips, cube rolls']}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Solution"}),": Implement ",(0,i.jsx)(e.strong,{children:"replanning on failure"})]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'while not task_complete:\r\n    plan = llm_planner.plan(goal, current_detections)\r\n    try:\r\n        executor.execute(plan)\r\n        task_complete = True\r\n    except ExecutionFailure as e:\r\n        # Update detections, replan\r\n        current_detections = vision_system.get_latest()\r\n        feedback = f"Previous plan failed: {e}. Replanning..."\n'})}),"\n",(0,i.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.strong,{children:"LLMs can decompose high-level goals into executable plans"})}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Prompt engineering is critical"})," \u2014 constraints and examples matter"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Plans must be validated"})," \u2014 LLMs hallucinate, we check physics/constraints"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Caching and local models"})," reduce latency on real robots"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Real robots need replanning"})," \u2014 execution failures are common"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human oversight"})," essential for safety-critical applications"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(e.p,{children:["In ",(0,i.jsx)(e.strong,{children:"Lesson 5"}),", you'll implement the ",(0,i.jsx)(e.strong,{children:"manipulation pipeline"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:'Grasp planning: "How should the robot hold this object?"'}),"\n",(0,i.jsx)(e.li,{children:'Motion planning: "What path should the arm take?"'}),"\n",(0,i.jsx)(e.li,{children:'Force control: "How hard should the gripper squeeze?"'}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Combining planning (Lesson 4) + manipulation (Lesson 5) = autonomous grasping!"}),"\n",(0,i.jsx)(e.p,{children:"See you in Lesson 5! \ud83e\udd16\u270b"})]})}function p(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}}}]);