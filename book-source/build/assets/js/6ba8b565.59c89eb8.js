"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[168],{4515:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-4-vla/lesson-01","title":"Lesson 1: Vision-Language-Action Models","description":"This lesson covers multimodal AI models for robotics.","source":"@site/docs/chapter-4-vla/lesson-01.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/lesson-01","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-vla/lesson-01","draft":false,"unlisted":false,"editUrl":"https://github.com/devhammad0/physical-ai-robotics-textbook/tree/main/docs/chapter-4-vla/lesson-01.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Vision-Language-Action Models","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-vla/intro"},"next":{"title":"Lesson 2: Vision-Language-Action Models","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-vla/lesson-02"}}');var o=i(6070),t=i(8439);const r={sidebar_position:2},l="Lesson 1: Vision-Language-Action Models",a={},c=[{value:"Overview",id:"overview",level:2},{value:"Content",id:"content",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"lesson-1-vision-language-action-models",children:"Lesson 1: Vision-Language-Action Models"})}),"\n",(0,o.jsx)(n.p,{children:"This lesson covers multimodal AI models for robotics."}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Topic: Vision-Language-Action Models"}),"\n",(0,o.jsx)(n.li,{children:"Duration: 45 minutes"}),"\n",(0,o.jsx)(n.li,{children:"Difficulty: Advanced"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"content",children:"Content"}),"\n",(0,o.jsx)(n.p,{children:"Placeholder for Lesson 1 content. This will be populated during Phase 2."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand VLA model architectures"}),"\n",(0,o.jsx)(n.li,{children:"Deploy language-vision models"}),"\n",(0,o.jsx)(n.li,{children:"Build end-to-end robotic systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2010.11929",children:"Vision Transformers Research"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/search/",children:"Multimodal Models for Robotics"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8439:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(758);const o={},t=s.createContext(o);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);