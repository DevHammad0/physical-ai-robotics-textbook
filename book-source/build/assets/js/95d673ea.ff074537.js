"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[2707],{236:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-3-autonomous-navigation/multi-sensor-perception-and-fusion","title":"Lesson 7: Multi-Sensor Perception and Fusion","description":"Duration Unlimited | Priority Lesson 1 or 3","source":"@site/docs/03-chapter-3-autonomous-navigation/07-multi-sensor-perception-and-fusion.md","sourceDirName":"03-chapter-3-autonomous-navigation","slug":"/chapter-3-autonomous-navigation/multi-sensor-perception-and-fusion","permalink":"/physical-ai-robotics-textbook/docs/chapter-3-autonomous-navigation/multi-sensor-perception-and-fusion","draft":false,"unlisted":false,"editUrl":"https://github.com/devhammad0/physical-ai-robotics-textbook/tree/main/docs/03-chapter-3-autonomous-navigation/07-multi-sensor-perception-and-fusion.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6: Autonomous Humanoid Navigation","permalink":"/physical-ai-robotics-textbook/docs/chapter-3-autonomous-navigation/autonomous-humanoid-navigation"},"next":{"title":"Lesson 8: Capstone - Autonomous Navigation End-to-End","permalink":"/physical-ai-robotics-textbook/docs/chapter-3-autonomous-navigation/capstone-mission"}}');var r=i(6070),o=i(8439);const t={},a="Lesson 7: Multi-Sensor Perception and Fusion",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Layer 1: Foundation",id:"layer-1-foundation",level:2},{value:"7.1 Sensor Characteristics",id:"71-sensor-characteristics",level:3},{value:"Camera (Visual)",id:"camera-visual",level:4},{value:"LiDAR (Light Detection and Ranging)",id:"lidar-light-detection-and-ranging",level:4},{value:"IMU (Accelerometer + Gyroscope)",id:"imu-accelerometer--gyroscope",level:4},{value:"7.2 Sensor Fusion Motivation",id:"72-sensor-fusion-motivation",level:3},{value:"7.3 Fusion Architectures",id:"73-fusion-architectures",level:3},{value:"7.4 Covariance-Weighted Fusion",id:"74-covariance-weighted-fusion",level:3},{value:"Layer 2: Collaboration (ROS 2 Integration)",id:"layer-2-collaboration-ros-2-integration",level:2},{value:"2.1 Temporal Synchronization",id:"21-temporal-synchronization",level:3},{value:"2.2 Using message_filters for Synchronization",id:"22-using-message_filters-for-synchronization",level:3},{value:"2.3 Sensor Fusion Node Architecture",id:"23-sensor-fusion-node-architecture",level:3},{value:"Layer 3: Intelligence (Fusion Tuning)",id:"layer-3-intelligence-fusion-tuning",level:2},{value:"3.1 Covariance Tuning",id:"31-covariance-tuning",level:3},{value:"3.2 Failure Mode Handling",id:"32-failure-mode-handling",level:3},{value:"3.3 Measuring Fusion Performance",id:"33-measuring-fusion-performance",level:3},{value:"Layer 4: Advanced",id:"layer-4-advanced",level:2},{value:"4.1 Extended Kalman Filter (EKF) Fusion",id:"41-extended-kalman-filter-ekf-fusion",level:3},{value:"Summary",id:"summary",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Example 1: Multi-Sensor Fusion Node",id:"example-1-multi-sensor-fusion-node",level:3},{value:"Example 2: Sensor Validation",id:"example-2-sensor-validation",level:3},{value:"Practice Exercise",id:"practice-exercise",level:2},{value:"References",id:"references",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lesson-7-multi-sensor-perception-and-fusion",children:"Lesson 7: Multi-Sensor Perception and Fusion"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 3 hours | ",(0,r.jsx)(n.strong,{children:"Level"}),": Unlimited | ",(0,r.jsx)(n.strong,{children:"Priority"}),": P3 | ",(0,r.jsx)(n.strong,{children:"Prerequisite"}),": Lesson 1 or 3"]}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explain"})," sensor characteristics (camera, LiDAR, IMU) and their trade-offs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement"})," temporal synchronization using ROS 2 message_filters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Design"})," covariance-weighted sensor fusion strategies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Measure"})," accuracy improvements from multi-sensor fusion (>20% target)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Handle"})," sensor failures and gracefully degrade"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"layer-1-foundation",children:"Layer 1: Foundation"}),"\n",(0,r.jsx)(n.h3,{id:"71-sensor-characteristics",children:"7.1 Sensor Characteristics"}),"\n",(0,r.jsx)(n.h4,{id:"camera-visual",children:"Camera (Visual)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Strengths:\n\u251c\u2500 Rich visual information (features, colors, textures)\n\u251c\u2500 Works well in daylight/indoor with good lighting\n\u2514\u2500 Scalable (multiple cameras cheap)\n\nWeaknesses:\n\u251c\u2500 Fails in darkness or low contrast\n\u251c\u2500 Motion blur at high speed\n\u251c\u2500 Computationally expensive feature extraction\n\u2514\u2500 Sensitive to lighting changes\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Good for"}),": Feature-based SLAM, object detection"]}),"\n",(0,r.jsx)(n.h4,{id:"lidar-light-detection-and-ranging",children:"LiDAR (Light Detection and Ranging)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Strengths:\n\u251c\u2500 Robust distance measurement (works day/night)\n\u251c\u2500 3D point cloud, good range (5-30m)\n\u251c\u2500 Less affected by lighting\n\u2514\u2500 Direct depth, no scale ambiguity\n\nWeaknesses:\n\u251c\u2500 Sparse point clouds (vs. dense images)\n\u251c\u2500 Expensive hardware\n\u251c\u2500 Doesn't capture color/texture\n\u2514\u2500 Limited by range and reflectivity\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Good for"}),": Range sensing, obstacle detection"]}),"\n",(0,r.jsx)(n.h4,{id:"imu-accelerometer--gyroscope",children:"IMU (Accelerometer + Gyroscope)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Strengths:\n\u251c\u2500 Immediate motion feedback (no latency)\n\u251c\u2500 Works anywhere (indoors/outdoors, dark/light)\n\u251c\u2500 Small, cheap, lightweight\n\u2514\u2500 No external dependencies\n\nWeaknesses:\n\u251c\u2500 Accelerometer drifts (integration error accumulates)\n\u251c\u2500 Gyroscope drifts (bias accumulates)\n\u251c\u2500 Noisy measurements\n\u2514\u2500 Can't estimate absolute position alone\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Good for"}),": Motion tracking, orientation"]}),"\n",(0,r.jsx)(n.h3,{id:"72-sensor-fusion-motivation",children:"7.2 Sensor Fusion Motivation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Single sensor limitations"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Camera in dark room:\n\u251c\u2500 Can't see features \u2192 SLAM fails\n\u2514\u2500 Output: No pose estimate\n\nLiDAR in featureless corridor:\n\u251c\u2500 All similar point clouds \u2192 ambiguous\n\u2514\u2500 Output: Multiple possible poses\n\nIMU alone:\n\u251c\u2500 Drifts over time\n\u2514\u2500 Output: Rough estimate, diverges\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Fused sensors"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Camera + LiDAR + IMU:\n\u251c\u2500 Camera: rich features (when light available)\n\u251c\u2500 LiDAR: robust range (works day/night)\n\u251c\u2500 IMU: immediate motion feedback\n\u2514\u2500 Output: Robust pose with 20%+ accuracy improvement\n"})}),"\n",(0,r.jsx)(n.h3,{id:"73-fusion-architectures",children:"7.3 Fusion Architectures"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tightly-coupled fusion"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"All sensors feed into single optimizer"}),"\n",(0,r.jsx)(n.li,{children:"Best accuracy, most complex"}),"\n",(0,r.jsx)(n.li,{children:"Example: Visual-inertial SLAM"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Loosely-coupled fusion"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Each sensor produces independent pose estimate"}),"\n",(0,r.jsx)(n.li,{children:"Estimates combined via weighted average"}),"\n",(0,r.jsx)(n.li,{children:"Simpler, sufficient for most applications"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Our approach"}),": Loosely-coupled with covariance weighting"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Visual odometry \u2192 pose (with covariance)\n                      \u2502\n                      \u251c\u2192 Weighted fusion \u2192 final pose\n                      \u2502\nLiDAR odometry \u2192 pose (with covariance)\n                      \u2502\nIMU-based estimate \u2192 orientation (with covariance)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"74-covariance-weighted-fusion",children:"7.4 Covariance-Weighted Fusion"}),"\n",(0,r.jsx)(n.p,{children:"Each sensor produces:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State estimate"}),": Position and orientation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Covariance"}),": Uncertainty matrix (how confident?)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Fusion rule"})," (Kalman filter-like):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Measurement 1: pose_1, cov_1 (high confidence, low covariance)\nMeasurement 2: pose_2, cov_2 (low confidence, high covariance)\n\nFused estimate = (cov_2*pose_1 + cov_1*pose_2) / (cov_1 + cov_2)\n\nIntuition: Weight measurements by inverse covariance\n"})}),"\n",(0,r.jsx)(n.h2,{id:"layer-2-collaboration-ros-2-integration",children:"Layer 2: Collaboration (ROS 2 Integration)"}),"\n",(0,r.jsx)(n.h3,{id:"21-temporal-synchronization",children:"2.1 Temporal Synchronization"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Challenge"}),": Sensors have different frequencies and latencies"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Camera @ 30 Hz:    [    @0.1s   @0.2s   @0.3s   ]\nLiDAR @ 10 Hz:     [   @0.0s            @0.2s  ]\nIMU @ 100 Hz:      [@ @ @ @ @ @ @ @ @ @ @ @ @ @ ]\n\nHow do we combine at same time?\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": ",(0,r.jsx)(n.code,{children:"message_filters"})," with exact/approximate time synchronization"]}),"\n",(0,r.jsx)(n.h3,{id:"22-using-message_filters-for-synchronization",children:"2.2 Using message_filters for Synchronization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# code_examples/temporal_sync_handler.py\nimport rclpy\nfrom message_filters import Subscriber, ApproximateTimeSynchronizer\nfrom geometry_msgs.msg import PoseStamped\nfrom sensor_msgs.msg import Image, Imu\n\nclass SensorFusionNode(rclpy.Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n\n        # Create subscribers\n        visual_sub = Subscriber(self, PoseStamped, '/camera/odometry')\n        lidar_sub = Subscriber(self, PoseStamped, '/lidar/odometry')\n        imu_sub = Subscriber(self, Imu, '/imu/data')\n\n        # Synchronize messages\n        # ApproximateTimeSynchronizer allows time slew (0.1s tolerance)\n        sync = ApproximateTimeSynchronizer(\n            [visual_sub, lidar_sub, imu_sub],\n            queue_size=10,\n            slop=0.1  # 100ms tolerance\n        )\n\n        sync.registerCallback(self.fusion_callback)\n\n    def fusion_callback(self, visual, lidar, imu):\n        \"\"\"\n        Called when all three messages are available\n        within 100ms of each other\n        \"\"\"\n        self.fuse_measurements(visual, lidar, imu)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"23-sensor-fusion-node-architecture",children:"2.3 Sensor Fusion Node Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# sensor_fusion_node_config.yaml\nsensor_fusion:\n  input_topics:\n    visual_odometry: "/camera/odometry"  # from SLAM\n    lidar_odometry: "/lidar/odometry"    # from LiDAR-SLAM\n    imu_data: "/imu/data"               # raw IMU\n\n  output_topics:\n    fused_pose: "/fused/odometry"       # fused estimate\n    diagnostics: "/fused/diagnostics"\n\n  fusion_parameters:\n    use_visual: true                     # enable camera odometry\n    use_lidar: true                      # enable LiDAR odometry\n    use_imu: true                        # enable IMU\n\n    sync_tolerance: 0.1  # seconds (100ms)\n\n    # Weighting (lower covariance = higher weight)\n    visual_weight: 1.0   # reference weight\n    lidar_weight: 1.5    # LiDAR slightly less trusted\n    imu_weight: 2.0      # IMU least trusted for position\n\n  failure_modes:\n    visual_timeout: 2.0    # seconds until visual considered dead\n    lidar_timeout: 2.0\n    imu_timeout: 1.0\n    fallback_sensor: "lidar"  # use this if others fail\n'})}),"\n",(0,r.jsx)(n.h2,{id:"layer-3-intelligence-fusion-tuning",children:"Layer 3: Intelligence (Fusion Tuning)"}),"\n",(0,r.jsx)(n.h3,{id:"31-covariance-tuning",children:"3.1 Covariance Tuning"}),"\n",(0,r.jsx)(n.p,{children:"Each sensor publishes odometry with covariance matrix:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def compute_covariance(sensor_noise, measurement_error):\n    """\n    Covariance = measure of uncertainty\n\n    Low covariance (0.01) = high confidence\n    High covariance (1.0) = low confidence\n    """\n    return np.eye(3) * measurement_error\n\n# Visual odometry covariance\n# Usually low in well-lit scenes, high in darkness\nvisual_cov = 0.05  # confident in daylight\n\n# LiDAR odometry covariance\n# Usually low, except in featureless corridors\nlidar_cov = 0.10   # medium confidence\n\n# IMU covariance\n# High initially, increases with time (drift)\nimu_cov_initial = 0.5  # low confidence\nimu_cov_over_time = imu_cov_initial + drift_rate * time\n'})}),"\n",(0,r.jsx)(n.h3,{id:"32-failure-mode-handling",children:"3.2 Failure Mode Handling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# code_examples/fusion_failure_handling.py\nclass FusionWithFailover(rclpy.Node):\n    def __init__(self):\n        self.active_sensors = {'visual': True, 'lidar': True, 'imu': True}\n        self.last_message_time = {\n            'visual': None, 'lidar': None, 'imu': None\n        }\n\n    def check_sensor_health(self):\n        \"\"\"Check which sensors are producing data\"\"\"\n        current_time = self.get_clock().now()\n\n        for sensor in self.active_sensors.keys():\n            if self.last_message_time[sensor] is None:\n                continue\n\n            time_since_message = (current_time -\n                               self.last_message_time[sensor]).nanoseconds / 1e9\n\n            if time_since_message > self.timeout[sensor]:\n                self.get_logger().warn(f'{sensor} timeout - disabling')\n                self.active_sensors[sensor] = False\n            else:\n                self.active_sensors[sensor] = True\n\n    def select_fallback(self):\n        \"\"\"Use available sensor if others fail\"\"\"\n        if self.active_sensors['visual']:\n            return 'visual'\n        elif self.active_sensors['lidar']:\n            return 'lidar'\n        elif self.active_sensors['imu']:\n            return 'imu'\n        else:\n            return None  # all sensors failed!\n"})}),"\n",(0,r.jsx)(n.h3,{id:"33-measuring-fusion-performance",children:"3.3 Measuring Fusion Performance"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ground truth comparison"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Compare fused pose to ground truth\nerror_before = compute_ate(visual_only, ground_truth)\nerror_after = compute_ate(fused_pose, ground_truth)\n\nimprovement = (error_before - error_after) / error_before * 100\n# Target: >20% improvement\n"})}),"\n",(0,r.jsx)(n.h2,{id:"layer-4-advanced",children:"Layer 4: Advanced"}),"\n",(0,r.jsx)(n.h3,{id:"41-extended-kalman-filter-ekf-fusion",children:"4.1 Extended Kalman Filter (EKF) Fusion"}),"\n",(0,r.jsx)(n.p,{children:"More advanced than simple weighted averaging:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"EKF maintains:\n\u251c\u2500 State: [x, y, theta, vx, vy, vtheta]\n\u251c\u2500 Covariance: Uncertainty in each state variable\n\u2514\u2500 Update cycle:\n    1. Predict: Use motion model\n    2. Measure: Receive sensor measurements\n    3. Correct: Weight update by sensor covariance\n    4. Repeat\n"})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Concept"}),(0,r.jsx)(n.th,{children:"Key Insight"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Sensor fusion"})}),(0,r.jsx)(n.td,{children:"Combines strengths, mitigates weaknesses"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Covariance"})}),(0,r.jsx)(n.td,{children:"Measures uncertainty; inverse of confidence"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Synchronization"})}),(0,r.jsx)(n.td,{children:"Temporal alignment critical for fusion"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Loosely-coupled"})}),(0,r.jsx)(n.td,{children:"Simple, sufficient for most applications"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Failure handling"})}),(0,r.jsx)(n.td,{children:"Monitor sensor health, fallback gracefully"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsx)(n.h3,{id:"example-1-multi-sensor-fusion-node",children:"Example 1: Multi-Sensor Fusion Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# code_examples/sensor_fusion_node.py\nimport numpy as np\nimport rclpy\nfrom message_filters import Subscriber, ApproximateTimeSynchronizer\nfrom geometry_msgs.msg import PoseStamped\nfrom sensor_msgs.msg import Imu\nfrom nav_msgs.msg import Odometry\n\nclass MultiSensorFusion(rclpy.Node):\n    def __init__(self):\n        super().__init__('multi_sensor_fusion')\n\n        # Subscribers with synchronization\n        visual_sub = Subscriber(self, PoseStamped, '/camera/odometry')\n        lidar_sub = Subscriber(self, PoseStamped, '/lidar/odometry')\n        imu_sub = Subscriber(self, Imu, '/imu/data')\n\n        sync = ApproximateTimeSynchronizer(\n            [visual_sub, lidar_sub, imu_sub],\n            queue_size=10, slop=0.1\n        )\n        sync.registerCallback(self.fusion_callback)\n\n        self.fused_pub = self.create_publisher(\n            Odometry, '/fused/odometry', 10)\n\n    def fusion_callback(self, visual, lidar, imu):\n        \"\"\"Fuse three sensor modalities\"\"\"\n\n        # Extract covariances\n        visual_cov = 0.05  # high confidence\n        lidar_cov = 0.10   # medium confidence\n        imu_cov = 0.5      # low confidence for position\n\n        # Weight by inverse covariance\n        weights = np.array([1/visual_cov, 1/lidar_cov, 1/imu_cov])\n        weights /= weights.sum()  # normalize\n\n        # Weighted average of positions\n        positions = np.array([\n            [visual.pose.position.x, visual.pose.position.y],\n            [lidar.pose.position.x, lidar.pose.position.y],\n            [imu.orientation.x, imu.orientation.y]  # IMU limited\n        ])\n\n        fused_position = weights.reshape(-1, 1) * positions\n        fused_position = fused_position.sum(axis=0)\n\n        # Publish fused estimate\n        odometry = Odometry()\n        odometry.pose.pose.position.x = fused_position[0]\n        odometry.pose.pose.position.y = fused_position[1]\n        odometry.header.stamp = self.get_clock().now().to_msg()\n        odometry.header.frame_id = 'map'\n\n        self.fused_pub.publish(odometry)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"example-2-sensor-validation",children:"Example 2: Sensor Validation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# code_examples/validate_sensor_fusion_accuracy.py\nimport numpy as np\n\nclass FusionValidator(rclpy.Node):\n    def __init__(self):\n        super().__init__('fusion_validator')\n        self.single_sensor_errors = []\n        self.fused_errors = []\n\n    def evaluate_accuracy(self, single_sensor, fused, ground_truth):\n        \"\"\"\n        Measure improvement from fusion\n        \"\"\"\n        single_error = np.linalg.norm(single_sensor - ground_truth)\n        fused_error = np.linalg.norm(fused - ground_truth)\n\n        improvement = (single_error - fused_error) / single_error * 100\n\n        self.get_logger().info(\n            f'Single sensor error: {single_error:.3f}m\\n'\n            f'Fused error: {fused_error:.3f}m\\n'\n            f'Improvement: {improvement:.1f}%'\n        )\n\n        return improvement\n"})}),"\n",(0,r.jsx)(n.h2,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Implement multi-sensor fusion and verify >20% accuracy improvement"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Set up three odometry sources:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Visual odometry (camera-based SLAM)"}),"\n",(0,r.jsx)(n.li,{children:"LiDAR odometry (if available, or use simulated)"}),"\n",(0,r.jsx)(n.li,{children:"IMU (gyroscope-based orientation)"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Implement temporal synchronization"}),"\n",(0,r.jsx)(n.li,{children:"Create fusion node with weighted averaging"}),"\n",(0,r.jsxs)(n.li,{children:["Run test scenario in Gazebo:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Well-lit room (cameras work well)"}),"\n",(0,r.jsx)(n.li,{children:"Dark room (cameras fail, LiDAR works)"}),"\n",(0,r.jsx)(n.li,{children:"Feature-poor corridor (LiDAR ambiguous)"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Measure accuracy improvement"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Success criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fusion node runs without errors"}),"\n",(0,r.jsx)(n.li,{children:"Temporal synchronization working (topics synchronized)"}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"20% accuracy improvement over best single sensor"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Graceful failure handling (fallback when sensor dies)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Fusion Survey"}),": ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1902.01305",children:"Multi-Sensor Fusion Review"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Extended Kalman Filter"}),": ",(0,r.jsx)(n.a,{href:"https://www.kalmanfilter.net/multivariate_k.html",children:"EKF Tutorial"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 message_filters"}),": ",(0,r.jsx)(n.a,{href:"http://wiki.ros.org/message_filters",children:"Message Filters Documentation"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"You now understand multi-sensor fusion for robust perception. Choose your path:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["\u2192 ",(0,r.jsx)(n.a,{href:"/physical-ai-robotics-textbook/docs/chapter-3-autonomous-navigation/capstone-mission",children:"Lesson 8: Capstone"})]})," (Integrate complete system)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Lesson 7 Summary"}),": Multi-sensor fusion combines visual, range, and inertial information for >20% accuracy improvement. Temporal synchronization, covariance weighting, and failure handling enable robust autonomous navigation in diverse conditions."]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8439:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var s=i(758);const r={},o=s.createContext(r);function t(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);