"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[1201],{5551:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-4-ai-integration/lesson-3-vision-systems","title":"Lesson 3: Vision Systems for Robotics","description":"Learning Objectives","source":"@site/docs/03-chapter-4-ai-integration/03-lesson-3-vision-systems.md","sourceDirName":"03-chapter-4-ai-integration","slug":"/chapter-4-ai-integration/lesson-3-vision-systems","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-3-vision-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/devhammad0/physical-ai-robotics-textbook/tree/main/docs/03-chapter-4-ai-integration/03-lesson-3-vision-systems.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2: Voice Input with OpenAI Whisper","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-2-voice-whisper"},"next":{"title":"Lesson 4: LLM-Driven Planning & Task Decomposition","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-4-llm-planning"}}');var s=r(6070),t=r(8439);const o={},a="Lesson 3: Vision Systems for Robotics",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"What are Vision Systems?",id:"what-are-vision-systems",level:2},{value:"1. Object Detection: &quot;What objects are in the scene?&quot;",id:"1-object-detection-what-objects-are-in-the-scene",level:3},{value:"2. Semantic Segmentation: &quot;What is each pixel?&quot;",id:"2-semantic-segmentation-what-is-each-pixel",level:3},{value:"3. Depth Sensing: &quot;How far is everything?&quot;",id:"3-depth-sensing-how-far-is-everything",level:3},{value:"Vision Pipeline Architecture",id:"vision-pipeline-architecture",level:2},{value:"Object Detection: YOLO-v8",id:"object-detection-yolo-v8",level:2},{value:"What is YOLO?",id:"what-is-yolo",level:3},{value:"Installation",id:"installation",level:3},{value:"Basic Usage",id:"basic-usage",level:3},{value:"ROS 2 Detection Node",id:"ros-2-detection-node",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:2},{value:"What is Semantic Segmentation?",id:"what-is-semantic-segmentation",level:3},{value:"Installation",id:"installation-1",level:3},{value:"ROS 2 Segmentation Node",id:"ros-2-segmentation-node",level:3},{value:"Depth Sensing",id:"depth-sensing",level:2},{value:"Depth Cameras in Isaac Sim",id:"depth-cameras-in-isaac-sim",level:3},{value:"3D Object Localization",id:"3d-object-localization",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise L3-1: Object Detection",id:"exercise-l3-1-object-detection",level:3},{value:"Exercise L3-2: ROS 2 Vision Integration",id:"exercise-l3-2-ros-2-vision-integration",level:3},{value:"Real Hardware Considerations",id:"real-hardware-considerations",level:2},{value:"Differences from Simulation",id:"differences-from-simulation",level:3},{value:"Robustness Strategies",id:"robustness-strategies",level:3},{value:"Camera Setup",id:"camera-setup",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-3-vision-systems-for-robotics",children:"Lesson 3: Vision Systems for Robotics"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement object detection (YOLO-v8) to identify objects in scenes"}),"\n",(0,s.jsx)(n.li,{children:"Perform semantic segmentation to classify pixels by object type"}),"\n",(0,s.jsx)(n.li,{children:"Integrate depth data for 3D perception"}),"\n",(0,s.jsx)(n.li,{children:"Create ROS 2 nodes that publish detection and segmentation results"}),"\n",(0,s.jsx)(n.li,{children:"Understand real hardware challenges in vision (lighting, occlusion, object variability)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"what-are-vision-systems",children:"What are Vision Systems?"}),"\n",(0,s.jsxs)(n.p,{children:["A ",(0,s.jsx)(n.strong,{children:"vision system"}),' gives robots the ability to "see" and understand their environment. It answers three questions:']}),"\n",(0,s.jsx)(n.h3,{id:"1-object-detection-what-objects-are-in-the-scene",children:'1. Object Detection: "What objects are in the scene?"'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Input Image:    [Red cube, blue sphere, shelf, table, ...]\r\n                \u2193\r\nDetector (YOLO-v8)\r\n                \u2193\r\nOutput:         [{class: "cube", bbox: [100,50,150,100], confidence: 0.95},\r\n                 {class: "sphere", bbox: [200,80,250,130], confidence: 0.92},\r\n                 ...]\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Uses"}),": Identifying targets for manipulation, finding obstacles, scene inventory"]}),"\n",(0,s.jsx)(n.h3,{id:"2-semantic-segmentation-what-is-each-pixel",children:'2. Semantic Segmentation: "What is each pixel?"'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Input Image:    [Camera image - 640\xd7480 pixels]\r\n                \u2193\r\nSegmentation (FCN/DeepLab)\r\n                \u2193\r\nOutput:         [Label for each pixel: 0=background, 1=table, 2=cube, ...]\r\n                Visualization: Color-coded segmentation mask\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Uses"}),": Scene understanding, free space detection, grasping surface identification"]}),"\n",(0,s.jsx)(n.h3,{id:"3-depth-sensing-how-far-is-everything",children:'3. Depth Sensing: "How far is everything?"'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Input:          Stereo cameras or depth sensor\r\n                \u2193\r\nDepth Estimation\r\n                \u2193\r\nOutput:         [Depth for each pixel: distance from camera in meters]\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Uses"}),": 3D object localization, obstacle avoidance, grasp planning"]}),"\n",(0,s.jsx)(n.h2,{id:"vision-pipeline-architecture",children:"Vision Pipeline Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Camera Stream   \u2502 (RGB images @ 10+ Hz)\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502\r\n         \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Object Detection Node      \u2502\r\n\u2502  (YOLO-v8)                  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 \u2022 Load model                \u2502\r\n\u2502 \u2022 Run inference (<100ms)    \u2502\r\n\u2502 \u2022 Extract bounding boxes    \u2502\r\n\u2502 \u2022 Filter by confidence      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502 (/robot/detections)\r\n         \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Segmentation Node          \u2502\r\n\u2502  (FCN/DeepLab)              \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 \u2022 Load model                \u2502\r\n\u2502 \u2022 Run inference             \u2502\r\n\u2502 \u2022 Generate pixel labels     \u2502\r\n\u2502 \u2022 Map to class names        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n         \u2502 (/robot/segmentation)\r\n         \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Combined Perception Result  \u2502\r\n\u2502 \u2022 Detected objects           \u2502\r\n\u2502 \u2022 Scene segmentation         \u2502\r\n\u2502 \u2022 Ready for planning         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"object-detection-yolo-v8",children:"Object Detection: YOLO-v8"}),"\n",(0,s.jsx)(n.h3,{id:"what-is-yolo",children:"What is YOLO?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"YOLO"}),' = "You Only Look Once" \u2014 a real-time object detection framework']}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fast"}),": <50ms inference on consumer GPU"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accurate"}),": \u226585% mean Average Precision (mAP)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Easy to use"}),": Pre-trained models available"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Trainable"}),": Can fine-tune on custom objects"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install ultralytics torch torchvision\r\n\r\n# Verify\r\npython -c \"from ultralytics import YOLO; print('YOLO installed')\"\n"})}),"\n",(0,s.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\r\nimport cv2\r\n\r\n# Load pre-trained model\r\nmodel = YOLO('yolov8n.pt')  # nano model, fastest\r\n\r\n# Run detection on image\r\nresults = model('image.jpg')\r\n\r\n# Extract detections\r\nfor result in results:\r\n    for detection in result.boxes:\r\n        x1, y1, x2, y2 = detection.xyxy[0]\r\n        confidence = detection.conf[0]\r\n        class_id = int(detection.cls[0])\r\n        class_name = result.names[class_id]\r\n\r\n        print(f\"{class_name}: {confidence:.2f} at [{x1},{y1},{x2},{y2}]\")\n"})}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-detection-node",children:"ROS 2 Detection Node"}),"\n",(0,s.jsxs)(n.p,{children:["File: ",(0,s.jsx)(n.code,{children:"chapter4_vision/src/yolo_detector_node.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nfrom ultralytics import YOLO\r\nimport numpy as np\r\nfrom datetime import datetime\r\n\r\nclass YOLODetectorNode(Node):\r\n    def __init__(self):\r\n        super().__init__('yolo_detector_node')\r\n\r\n        # Load YOLO model\r\n        self.get_logger().info('Loading YOLO model...')\r\n        self.model = YOLO('yolov8n.pt')  # nano model\r\n        self.get_logger().info('YOLO model loaded')\r\n\r\n        # Create subscribers and publishers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/robot/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.detections_pub = self.create_publisher(\r\n            String,  # For simplicity, publish as JSON string\r\n            '/robot/detections',\r\n            10\r\n        )\r\n\r\n        self.annotated_image_pub = self.create_publisher(\r\n            Image,\r\n            '/robot/camera/annotated',\r\n            10\r\n        )\r\n\r\n        self.cv_bridge = CvBridge()\r\n        self.confidence_threshold = 0.5\r\n\r\n        self.get_logger().info('YOLO Detector Node initialized')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process incoming camera image\"\"\"\r\n        try:\r\n            # Convert ROS image to OpenCV format\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\r\n\r\n            # Run YOLO inference\r\n            results = self.model(cv_image)\r\n\r\n            # Extract detections\r\n            detections = []\r\n            for result in results:\r\n                for detection in result.boxes:\r\n                    conf = float(detection.conf[0])\r\n\r\n                    # Filter by confidence threshold\r\n                    if conf < self.confidence_threshold:\r\n                        continue\r\n\r\n                    # Extract bounding box\r\n                    bbox = detection.xyxy[0]\r\n                    x1, y1, x2, y2 = [int(v) for v in bbox]\r\n\r\n                    # Extract class info\r\n                    class_id = int(detection.cls[0])\r\n                    class_name = result.names[class_id]\r\n\r\n                    detection_dict = {\r\n                        'class': class_name,\r\n                        'confidence': round(conf, 3),\r\n                        'bbox': {\r\n                            'x1': x1, 'y1': y1,\r\n                            'x2': x2, 'y2': y2,\r\n                            'width': x2 - x1,\r\n                            'height': y2 - y1\r\n                        }\r\n                    }\r\n                    detections.append(detection_dict)\r\n\r\n                # Publish detections as JSON\r\n                import json\r\n                msg_out = String()\r\n                msg_out.data = json.dumps({\r\n                    'detections': detections,\r\n                    'timestamp': datetime.now().isoformat(),\r\n                    'image_size': list(cv_image.shape)\r\n                })\r\n                self.detections_pub.publish(msg_out)\r\n\r\n                # Annotate image and publish\r\n                annotated = self._annotate_image(cv_image, detections)\r\n                annotated_msg = self.cv_bridge.cv2_to_imgmsg(annotated, encoding='bgr8')\r\n                self.annotated_image_pub.publish(annotated_msg)\r\n\r\n                self.get_logger().info(f'Detected {len(detections)} objects')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing image: {str(e)}')\r\n\r\n    def _annotate_image(self, image, detections):\r\n        \"\"\"Draw bounding boxes on image\"\"\"\r\n        annotated = image.copy()\r\n\r\n        for det in detections:\r\n            bbox = det['bbox']\r\n            x1, y1 = bbox['x1'], bbox['y1']\r\n            x2, y2 = bbox['x2'], bbox['y2']\r\n            label = f\"{det['class']} {det['confidence']:.2f}\"\r\n\r\n            # Draw bounding box\r\n            cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\r\n\r\n            # Draw label\r\n            cv2.putText(annotated, label, (x1, y1 - 10),\r\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n\r\n        return annotated\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    detector_node = YOLODetectorNode()\r\n    rclpy.spin(detector_node)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,s.jsx)(n.h3,{id:"what-is-semantic-segmentation",children:"What is Semantic Segmentation?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Semantic segmentation"})," assigns a class label to every pixel in the image."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Input Image:        [640\xd7480 RGB image]\r\n                    \u2193\r\nSegmentation Model  (FCN or DeepLab)\r\n                    \u2193\r\nOutput Mask:        [640\xd7480 array with class IDs]\r\nVisualization:      [Color-coded mask showing regions]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"installation-1",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install segmentation-models-pytorch albumentations\r\n\r\n# Or use pre-trained models from torchvision\r\npip install torch torchvision\n"})}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-segmentation-node",children:"ROS 2 Segmentation Node"}),"\n",(0,s.jsxs)(n.p,{children:["File: ",(0,s.jsx)(n.code,{children:"chapter4_vision/src/segmentation_node.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nfrom torchvision.models.segmentation import fcn_resnet50\r\nimport numpy as np\r\nimport json\r\n\r\nclass SegmentationNode(Node):\r\n    def __init__(self):\r\n        super().__init__('segmentation_node')\r\n\r\n        # Load segmentation model\r\n        self.get_logger().info('Loading segmentation model...')\r\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n        self.model = fcn_resnet50(pretrained=True).to(self.device)\r\n        self.model.eval()\r\n        self.get_logger().info(f'Segmentation model loaded on {self.device}')\r\n\r\n        # Class mapping for COCO dataset\r\n        self.coco_classes = {\r\n            0: 'background', 1: 'person', 2: 'bicycle', 3: 'car',\r\n            # ... (21 classes total)\r\n            15: 'cat', 16: 'dog', 18: 'table', 19: 'chair', 20: 'other'\r\n        }\r\n\r\n        # Create subscribers and publishers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/robot/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.segmentation_pub = self.create_publisher(\r\n            Image,\r\n            '/robot/segmentation',\r\n            10\r\n        )\r\n\r\n        self.cv_bridge = CvBridge()\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Process incoming camera image\"\"\"\r\n        try:\r\n            # Convert ROS image to OpenCV format\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\r\n\r\n            # Prepare image for model\r\n            image_tensor = self._prepare_image(cv_image)\r\n\r\n            # Run segmentation inference\r\n            with torch.no_grad():\r\n                output = self.model(image_tensor)\r\n\r\n            # Extract segmentation mask\r\n            segmentation = output['out'][0].argmax(dim=0).cpu().numpy()\r\n\r\n            # Create color-coded visualization\r\n            colored_mask = self._colorize_segmentation(segmentation)\r\n\r\n            # Publish segmentation mask\r\n            seg_msg = self.cv_bridge.cv2_to_imgmsg(colored_mask, encoding='rgb8')\r\n            self.segmentation_pub.publish(seg_msg)\r\n\r\n            self.get_logger().info('Published segmentation mask')\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error in segmentation: {str(e)}')\r\n\r\n    def _prepare_image(self, cv_image):\r\n        \"\"\"Prepare image for segmentation model\"\"\"\r\n        # Resize to 512\xd7512 (model input size)\r\n        resized = cv2.resize(cv_image, (512, 512))\r\n\r\n        # Normalize\r\n        normalized = resized.astype(np.float32) / 255.0\r\n        normalized = (normalized - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\r\n\r\n        # Convert to tensor\r\n        tensor = torch.from_numpy(normalized.transpose(2, 0, 1)).unsqueeze(0)\r\n        return tensor.to(self.device)\r\n\r\n    def _colorize_segmentation(self, segmentation):\r\n        \"\"\"Create color-coded segmentation visualization\"\"\"\r\n        h, w = segmentation.shape\r\n        colored = np.zeros((h, w, 3), dtype=np.uint8)\r\n\r\n        # Color map for different classes\r\n        colors = {\r\n            0: (0, 0, 0),           # background - black\r\n            15: (255, 0, 0),        # cat - red\r\n            16: (0, 255, 0),        # dog - green\r\n            18: (0, 0, 255),        # table - blue\r\n            19: (255, 255, 0),      # chair - yellow\r\n        }\r\n\r\n        for class_id, color in colors.items():\r\n            mask = (segmentation == class_id)\r\n            colored[mask] = color\r\n\r\n        return colored\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    seg_node = SegmentationNode()\r\n    rclpy.spin(seg_node)\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"depth-sensing",children:"Depth Sensing"}),"\n",(0,s.jsx)(n.h3,{id:"depth-cameras-in-isaac-sim",children:"Depth Cameras in Isaac Sim"}),"\n",(0,s.jsx)(n.p,{children:"Isaac Sim can simulate depth cameras (RGB-D sensors):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Isaac Sim provides depth as float32 array\r\ndepth_image = get_depth_from_camera()  # meters\r\n\r\n# Convert to visualization\r\ndepth_visual = (depth_image * 1000).astype(np.uint8)  # millimeters\r\ndepth_visual = cv2.applyColorMap(depth_visual, cv2.COLORMAP_TURBO)\r\n\r\n# Use depth for 3D object localization\r\n# If object center is at pixel (x, y) and depth is d:\r\n# 3D position = camera.project_pixel_to_3d(x, y, d)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3d-object-localization",children:"3D Object Localization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def get_object_3d_position(detection, depth_image, camera_intrinsics):\r\n    \"\"\"Convert 2D detection + depth to 3D position\"\"\"\r\n    # Get bounding box center\r\n    bbox = detection['bbox']\r\n    center_x = (bbox['x1'] + bbox['x2']) / 2\r\n    center_y = (bbox['y1'] + bbox['y2']) / 2\r\n\r\n    # Get depth at center\r\n    depth = depth_image[int(center_y), int(center_x)]\r\n\r\n    # Project to 3D using camera intrinsics\r\n    fx, fy, cx, cy = camera_intrinsics\r\n    x_3d = (center_x - cx) * depth / fx\r\n    y_3d = (center_y - cy) * depth / fy\r\n    z_3d = depth\r\n\r\n    return [x_3d, y_3d, z_3d]  # meters\n"})}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-l3-1-object-detection",children:"Exercise L3-1: Object Detection"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Detect objects in a scene using YOLO-v8"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Install YOLO: ",(0,s.jsx)(n.code,{children:"pip install ultralytics"})]}),"\n",(0,s.jsxs)(n.li,{children:["Download model: ",(0,s.jsx)(n.code,{children:"model = YOLO('yolov8n.pt')"})]}),"\n",(0,s.jsxs)(n.li,{children:["Detect in image: ",(0,s.jsx)(n.code,{children:"results = model('image.jpg')"})]}),"\n",(0,s.jsx)(n.li,{children:"Print detections with confidence scores"}),"\n",(0,s.jsx)(n.li,{children:"Visualize with bounding boxes"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2713 YOLO model loads and runs"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Detects \u22653 objects with confidence scores"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Bounding boxes correct (match visual inspection)"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Inference time <100ms"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-l3-2-ros-2-vision-integration",children:"Exercise L3-2: ROS 2 Vision Integration"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Create vision nodes that publish to ROS 2 topics"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create ",(0,s.jsx)(n.code,{children:"yolo_detector_node.py"})," subscribing to ",(0,s.jsx)(n.code,{children:"/robot/camera/image_raw"})]}),"\n",(0,s.jsxs)(n.li,{children:["Publish detections to ",(0,s.jsx)(n.code,{children:"/robot/detections"})]}),"\n",(0,s.jsx)(n.li,{children:"Launch Isaac Sim with camera"}),"\n",(0,s.jsx)(n.li,{children:"Run detector node"}),"\n",(0,s.jsxs)(n.li,{children:["Monitor output: ",(0,s.jsx)(n.code,{children:"ros2 topic echo /robot/detections"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2713 Nodes launch without errors"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Subscribe to camera topic successfully"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Publish detection JSON with objects + confidence"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Latency <200ms (inference + ROS overhead)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"real-hardware-considerations",children:"Real Hardware Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"differences-from-simulation",children:"Differences from Simulation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simulation (Isaac Sim)"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Perfect lighting, no shadows or glare"}),"\n",(0,s.jsx)(n.li,{children:"Known object dimensions and colors"}),"\n",(0,s.jsx)(n.li,{children:"No occlusion or overlapping objects"}),"\n",(0,s.jsx)(n.li,{children:"Camera calibration perfect"}),"\n",(0,s.jsx)(n.li,{children:"No sensor noise"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real Hardware"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Lighting varies: indoor/outdoor, shadows, reflections"}),"\n",(0,s.jsx)(n.li,{children:"Objects partially occluded by other objects"}),"\n",(0,s.jsx)(n.li,{children:"Camera calibration drift over time"}),"\n",(0,s.jsx)(n.li,{children:"Sensor noise: electrical noise in images"}),"\n",(0,s.jsx)(n.li,{children:"Object appearance changes: different angles, worn surfaces"}),"\n",(0,s.jsx)(n.li,{children:"Real-time constraints: must process at robot control rate"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"robustness-strategies",children:"Robustness Strategies"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data augmentation"}),": Train on varied lighting, angles, scales"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multiple detectors"}),": Use ensemble of YOLO + other models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth validation"}),": Check depth values for outliers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temporal filtering"}),": Smooth detections across frames"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Confidence thresholding"}),": Only trust high-confidence detections"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback behaviors"}),": What to do if vision fails?"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"camera-setup",children:"Camera Setup"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"For this course (Simulation)"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Isaac Sim provides perfect RGB-D cameras"}),"\n",(0,s.jsx)(n.li,{children:"Configure camera position on humanoid robot"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"For real robots"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Mount camera on robot wrist or head"}),"\n",(0,s.jsx)(n.li,{children:"Ensure stable mounting (no vibration)"}),"\n",(0,s.jsx)(n.li,{children:"Calibrate camera intrinsics once"}),"\n",(0,s.jsx)(n.li,{children:"Regular cleaning (dust on lens degrades performance)"}),"\n",(0,s.jsx)(n.li,{children:"Consider wide-angle lens for larger field of view"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Object detection (YOLO) is fast and accurate for real-time robotics"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Semantic segmentation gives pixel-level scene understanding"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Depth sensing enables 3D object localization"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"ROS 2 nodes can integrate deep learning models easily"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Real hardware has significantly different challenges (lighting, occlusion, noise)"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Confidence thresholding and error handling are critical"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["In ",(0,s.jsx)(n.strong,{children:"Lesson 4"}),", you'll integrate vision with ",(0,s.jsx)(n.strong,{children:"LLM-based planning"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use detected objects as input to LLM"}),"\n",(0,s.jsx)(n.li,{children:"LLM plans how to manipulate specific objects"}),"\n",(0,s.jsx)(n.li,{children:"Voice + Vision + Planning together"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"See you in Lesson 4! \ud83e\udde0\ud83c\udfac"})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8439:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var i=r(758);const s={},t=i.createContext(s);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);