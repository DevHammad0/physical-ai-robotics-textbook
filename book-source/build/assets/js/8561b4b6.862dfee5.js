"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[7531],{8439:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var s=i(758);const r={},o=s.createContext(r);function t(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(o.Provider,{value:n},e.children)}},9964:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"chapter-4-ai-integration/lesson-2-voice-whisper","title":"Lesson 2: Voice Input with OpenAI Whisper","description":"Learning Objectives","source":"@site/docs/03-chapter-4-ai-integration/02-lesson-2-voice-whisper.md","sourceDirName":"03-chapter-4-ai-integration","slug":"/chapter-4-ai-integration/lesson-2-voice-whisper","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-2-voice-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/devhammad0/physical-ai-robotics-textbook/tree/main/docs/03-chapter-4-ai-integration/02-lesson-2-voice-whisper.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1: Vision-Language-Action Architecture Overview","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-1-vla-architecture"},"next":{"title":"Lesson 3: Vision Systems for Robotics","permalink":"/physical-ai-robotics-textbook/docs/chapter-4-ai-integration/lesson-3-vision-systems"}}');var r=i(6070),o=i(8439);const t={},l="Lesson 2: Voice Input with OpenAI Whisper",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"What is Whisper?",id:"what-is-whisper",level:2},{value:"Two Deployment Options",id:"two-deployment-options",level:3},{value:"Option 1: Local Inference (Recommended for Chapter 4)",id:"option-1-local-inference-recommended-for-chapter-4",level:4},{value:"Option 2: OpenAI API (Cloud-based)",id:"option-2-openai-api-cloud-based",level:4},{value:"Real-Time vs. Batch Processing",id:"real-time-vs-batch-processing",level:2},{value:"Real-Time Processing",id:"real-time-processing",level:3},{value:"Batch Processing",id:"batch-processing",level:3},{value:"Architecture: Whisper ROS 2 Node",id:"architecture-whisper-ros-2-node",level:2},{value:"Step 1: Install Whisper",id:"step-1-install-whisper",level:2},{value:"Local Installation (Recommended)",id:"local-installation-recommended",level:3},{value:"Verify Whisper Works",id:"verify-whisper-works",level:3},{value:"Model Selection",id:"model-selection",level:3},{value:"Step 2: Create ROS 2 Whisper Node",id:"step-2-create-ros-2-whisper-node",level:2},{value:"Step 3: Set Up Audio Device",id:"step-3-set-up-audio-device",level:2},{value:"Check Available Audio Devices",id:"check-available-audio-devices",level:3},{value:"Configure Microphone",id:"configure-microphone",level:3},{value:"Test Audio Recording",id:"test-audio-recording",level:3},{value:"Step 4: Error Handling &amp; Edge Cases",id:"step-4-error-handling--edge-cases",level:2},{value:"No Audio Detected",id:"no-audio-detected",level:3},{value:"Transcription Confidence Too Low",id:"transcription-confidence-too-low",level:3},{value:"Timeout Handling",id:"timeout-handling",level:3},{value:"Step 5: Test the Node",id:"step-5-test-the-node",level:2},{value:"Create Test Script",id:"create-test-script",level:3},{value:"Run Tests",id:"run-tests",level:3},{value:"Exercise: Build Your Own Voice Node",id:"exercise-build-your-own-voice-node",level:2},{value:"Exercise L2-1: Transcribe Voice Command",id:"exercise-l2-1-transcribe-voice-command",level:3},{value:"Exercise L2-2: ROS 2 Node Integration",id:"exercise-l2-2-ros-2-node-integration",level:3},{value:"Real Hardware Considerations",id:"real-hardware-considerations",level:2},{value:"Differences from Simulation",id:"differences-from-simulation",level:3},{value:"Best Practices",id:"best-practices",level:3},{value:"Production Considerations",id:"production-considerations",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lesson-2-voice-input-with-openai-whisper",children:"Lesson 2: Voice Input with OpenAI Whisper"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lesson, you will:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Set up OpenAI Whisper for speech recognition (local inference or API)"}),"\n",(0,r.jsx)(n.li,{children:"Create a ROS 2 node that captures audio and publishes transcribed commands"}),"\n",(0,r.jsx)(n.li,{children:"Handle transcription errors and edge cases gracefully"}),"\n",(0,r.jsx)(n.li,{children:"Integrate voice input into the ROS 2 topic ecosystem"}),"\n",(0,r.jsx)(n.li,{children:"Understand real hardware considerations for microphone input and acoustic environments"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"what-is-whisper",children:"What is Whisper?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Whisper"})," is OpenAI's automatic speech recognition (ASR) system. It's:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robust"}),": Handles accents, background noise, technical language"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multilingual"}),": Supports 99 languages (Chapter 4 uses English only)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Open-source"}),": Available on GitHub, runnable locally"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Free"}),": No API charges for local inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accurate"}),": \u226590% accuracy on robot command vocabulary"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"two-deployment-options",children:"Two Deployment Options"}),"\n",(0,r.jsx)(n.h4,{id:"option-1-local-inference-recommended-for-chapter-4",children:"Option 1: Local Inference (Recommended for Chapter 4)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Microphone Audio \u2192 Whisper Model (Local) \u2192 Text\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency"}),": <500ms (fast enough for real-time)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy"}),": Audio never leaves your computer"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": Free (one-time model download)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Requirements"}),": 4GB+ free disk space, moderate CPU/GPU"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"option-2-openai-api-cloud-based",children:"Option 2: OpenAI API (Cloud-based)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Microphone Audio \u2192 OpenAI API \u2192 Text\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency"}),": 1-3 seconds (network dependent)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy"}),": Audio sent to OpenAI servers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": $0.02 per minute of audio"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Advantage"}),": No local compute needed"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For this course"}),", we recommend ",(0,r.jsx)(n.strong,{children:"local inference"})," for faster iteration and offline capability."]}),"\n",(0,r.jsx)(n.h2,{id:"real-time-vs-batch-processing",children:"Real-Time vs. Batch Processing"}),"\n",(0,r.jsx)(n.h3,{id:"real-time-processing",children:"Real-Time Processing"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'User speaks: "Find the red cube"'}),"\n",(0,r.jsx)(n.li,{children:"As they finish speaking, transcription begins"}),"\n",(0,r.jsx)(n.li,{children:"Result ready within 500ms"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Used in"}),": Interactive systems, command-and-control"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"User speaks and stops"}),"\n",(0,r.jsx)(n.li,{children:"Full audio captured"}),"\n",(0,r.jsx)(n.li,{children:"Whisper processes entire recording"}),"\n",(0,r.jsx)(n.li,{children:"Result ready after full processing"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Used in"}),": Non-interactive logging, archival"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"This lesson uses batch processing"})," (simpler for learning), but production systems often use streaming."]}),"\n",(0,r.jsx)(n.h2,{id:"architecture-whisper-ros-2-node",children:"Architecture: Whisper ROS 2 Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Microphone  \u2502 (Audio device)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 (Raw audio stream)\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  whisper_ros2_node.py        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2022 Listen for audio signal    \u2502\n\u2502 \u2022 Capture audio bytes        \u2502\n\u2502 \u2022 Call Whisper inference     \u2502\n\u2502 \u2022 Parse transcription text   \u2502\n\u2502 \u2022 Validate confidence score  \u2502\n\u2502 \u2022 Publish to ROS 2 topic     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 (/robot/voice_command topic)\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ROS 2 Topic: voice_cmd  \u2502\n\u2502 - text: "find red cube" \u2502\n\u2502 - confidence: 0.95      \u2502\n\u2502 - timestamp: 123456789  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,r.jsx)(n.h2,{id:"step-1-install-whisper",children:"Step 1: Install Whisper"}),"\n",(0,r.jsx)(n.h3,{id:"local-installation-recommended",children:"Local Installation (Recommended)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install required packages\npip install openai-whisper numpy scipy soundfile\n\n# Download Whisper model (one-time, ~141MB for base model)\npython -c \"import whisper; whisper.load_model('base')\"\n\n# Verify installation\npython -c \"import whisper; print('Whisper installed successfully')\"\n"})}),"\n",(0,r.jsx)(n.h3,{id:"verify-whisper-works",children:"Verify Whisper Works"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import whisper\n\n# Load model\nmodel = whisper.load_model("base")\n\n# Test with sample audio\nresult = model.transcribe("audio_sample.mp3")\nprint(result["text"])  # Output: transcribed text\n'})}),"\n",(0,r.jsx)(n.h3,{id:"model-selection",children:"Model Selection"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Size"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Accuracy"}),(0,r.jsx)(n.th,{children:"VRAM"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"tiny"}),(0,r.jsx)(n.td,{children:"39M"}),(0,r.jsx)(n.td,{children:"\u26a1\u26a1\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"1GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"base"}),(0,r.jsx)(n.td,{children:"141M"}),(0,r.jsx)(n.td,{children:"\u26a1\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"1GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"small"}),(0,r.jsx)(n.td,{children:"244M"}),(0,r.jsx)(n.td,{children:"\u26a1"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"2GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"medium"}),(0,r.jsx)(n.td,{children:"769M"}),(0,r.jsx)(n.td,{children:"\ud83d\udc22"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"5GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"large"}),(0,r.jsx)(n.td,{children:"2.9GB"}),(0,r.jsx)(n.td,{children:"\ud83d\udc22\ud83d\udc22"}),(0,r.jsx)(n.td,{children:"\u2b50\u2b50\u2b50\u2b50\u2b50"}),(0,r.jsx)(n.td,{children:"10GB"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For this course"}),": Use ",(0,r.jsx)(n.code,{children:"base"})," model (good balance of speed and accuracy)"]}),"\n",(0,r.jsx)(n.h2,{id:"step-2-create-ros-2-whisper-node",children:"Step 2: Create ROS 2 Whisper Node"}),"\n",(0,r.jsxs)(n.p,{children:["File: ",(0,r.jsx)(n.code,{children:"chapter4_voice/src/whisper_ros2_node.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float32\nimport whisper\nimport sounddevice as sd\nimport numpy as np\nfrom datetime import datetime\nimport json\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_node')\n\n        # Load Whisper model\n        self.get_logger().info('Loading Whisper model...')\n        self.model = whisper.load_model(\"base\")\n        self.get_logger().info('Whisper model loaded')\n\n        # Create publishers\n        self.voice_command_pub = self.create_publisher(\n            String,\n            '/robot/voice_command',\n            10\n        )\n        self.confidence_pub = self.create_publisher(\n            Float32,\n            '/robot/voice_confidence',\n            10\n        )\n\n        # Parameters\n        self.sample_rate = 16000  # Whisper expects 16kHz audio\n        self.duration = 5.0  # Listen for 5 seconds max\n        self.confidence_threshold = 0.7  # Minimum confidence to publish\n\n        self.get_logger().info('Whisper ROS 2 node initialized')\n        self.get_logger().info(f'Listening for voice commands on {self.sample_rate}Hz')\n\n    def listen_and_transcribe(self):\n        \"\"\"Listen to microphone and transcribe audio\"\"\"\n        self.get_logger().info('Listening... (speak now)')\n\n        try:\n            # Record audio from microphone\n            audio_data = sd.rec(\n                int(self.duration * self.sample_rate),\n                samplerate=self.sample_rate,\n                channels=1,\n                dtype=np.float32\n            )\n            sd.wait()  # Wait for recording to finish\n\n            # Flatten to 1D array\n            audio_data = audio_data.flatten()\n\n            # Transcribe with Whisper\n            self.get_logger().info('Transcribing...')\n            result = self.model.transcribe(\n                audio_data,\n                language=\"en\",\n                fp16=False  # Use CPU-safe FP32\n            )\n\n            # Extract text and confidence\n            text = result.get(\"text\", \"\").strip()\n            # Estimate confidence from Whisper segments\n            confidence = self._calculate_confidence(result)\n\n            if not text:\n                self.get_logger().warn('No speech detected')\n                return\n\n            self.get_logger().info(f'Transcribed: \"{text}\" (confidence: {confidence:.2f})')\n\n            # Publish only if confidence meets threshold\n            if confidence >= self.confidence_threshold:\n                # Publish command\n                msg = String()\n                msg.data = text\n                self.voice_command_pub.publish(msg)\n\n                # Publish confidence\n                conf_msg = Float32()\n                conf_msg.data = confidence\n                self.confidence_pub.publish(conf_msg)\n\n                self.get_logger().info('Published to /robot/voice_command')\n            else:\n                self.get_logger().warn(\n                    f'Confidence {confidence:.2f} below threshold {self.confidence_threshold}'\n                )\n\n        except Exception as e:\n            self.get_logger().error(f'Transcription error: {str(e)}')\n\n    def _calculate_confidence(self, result):\n        \"\"\"Estimate confidence from Whisper result\"\"\"\n        # Whisper doesn't provide per-word confidence, estimate from segments\n        if \"segments\" in result and result[\"segments\"]:\n            # Average confidence across segments (Whisper's confidence metric)\n            confidences = [seg.get(\"confidence\", 0.5) for seg in result[\"segments\"]]\n            return sum(confidences) / len(confidences) if confidences else 0.5\n        return 0.5  # Default if unable to estimate\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    whisper_node = WhisperNode()\n\n    # Run once, then exit (for testing)\n    # In production, use a loop with button or voice activation\n    whisper_node.listen_and_transcribe()\n\n    # For continuous operation:\n    # rclpy.spin(whisper_node)\n\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"step-3-set-up-audio-device",children:"Step 3: Set Up Audio Device"}),"\n",(0,r.jsx)(n.h3,{id:"check-available-audio-devices",children:"Check Available Audio Devices"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'python -c "import sounddevice; print(sounddevice.query_devices())"\n'})}),"\n",(0,r.jsx)(n.p,{children:"Output will show all available microphones. Identify yours."}),"\n",(0,r.jsx)(n.h3,{id:"configure-microphone",children:"Configure Microphone"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# On Linux\nalsamixer  # Adjust microphone input level\n\n# On macOS\nSystem Preferences > Sound > Input > [Select Microphone]\n\n# On Windows\nSettings > Sound > Volume and device preferences > [Select Microphone]\n"})}),"\n",(0,r.jsx)(n.h3,{id:"test-audio-recording",children:"Test Audio Recording"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python -c \"\nimport sounddevice as sd\nimport numpy as np\n\n# Record 3 seconds\naudio = sd.rec(int(3 * 16000), samplerate=16000, channels=1, dtype=np.float32)\nsd.wait()\nprint(f'Recorded {len(audio)} samples')\nprint(f'Audio level: {np.max(np.abs(audio)):.3f}')  # Should be > 0.1 for audible speech\n\"\n"})}),"\n",(0,r.jsx)(n.h2,{id:"step-4-error-handling--edge-cases",children:"Step 4: Error Handling & Edge Cases"}),"\n",(0,r.jsx)(n.h3,{id:"no-audio-detected",children:"No Audio Detected"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if len(audio_data[audio_data > 0.01]) < 100:  # Less than 100 samples above noise\n    self.get_logger().warn('No speech detected - audio too quiet')\n    return\n"})}),"\n",(0,r.jsx)(n.h3,{id:"transcription-confidence-too-low",children:"Transcription Confidence Too Low"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"if confidence < threshold:\n    # Ask user to repeat\n    self.get_logger().warn(f'Could not understand: \"{text}\" - please repeat')\n    return\n"})}),"\n",(0,r.jsx)(n.h3,{id:"timeout-handling",children:"Timeout Handling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"try:\n    audio_data = sd.rec(..., timeout=5)  # Wait max 5 seconds\nexcept TimeoutError:\n    self.get_logger().error('Recording timeout - no input detected')\n"})}),"\n",(0,r.jsx)(n.h2,{id:"step-5-test-the-node",children:"Step 5: Test the Node"}),"\n",(0,r.jsx)(n.h3,{id:"create-test-script",children:"Create Test Script"}),"\n",(0,r.jsxs)(n.p,{children:["File: ",(0,r.jsx)(n.code,{children:"chapter4_voice/tests/whisper_test.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import pytest\nimport rclpy\nfrom std_msgs.msg import String\nfrom chapter4_voice.whisper_ros2_node import WhisperNode\n\ndef test_whisper_node_initialization():\n    """Test node initializes successfully"""\n    rclpy.init()\n    try:\n        node = WhisperNode()\n        assert node is not None\n        assert node.model is not None\n    finally:\n        rclpy.shutdown()\n\ndef test_confidence_calculation():\n    """Test confidence score calculation"""\n    rclpy.init()\n    try:\n        node = WhisperNode()\n\n        # Mock Whisper result\n        result = {\n            "text": "find red cube",\n            "segments": [\n                {"confidence": 0.95},\n                {"confidence": 0.92},\n            ]\n        }\n\n        confidence = node._calculate_confidence(result)\n        assert 0.9 < confidence < 1.0\n    finally:\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    pytest.main([__file__, \'-v\'])\n'})}),"\n",(0,r.jsx)(n.h3,{id:"run-tests",children:"Run Tests"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ros2_packages/chapter4_voice\ncolcon build --packages-select chapter4_voice\ncolcon test --packages-select chapter4_voice\n"})}),"\n",(0,r.jsx)(n.h2,{id:"exercise-build-your-own-voice-node",children:"Exercise: Build Your Own Voice Node"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-l2-1-transcribe-voice-command",children:"Exercise L2-1: Transcribe Voice Command"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Set up Whisper and transcribe spoken commands"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Install Whisper: ",(0,r.jsx)(n.code,{children:"pip install openai-whisper"})]}),"\n",(0,r.jsxs)(n.li,{children:["Download model: ",(0,r.jsx)(n.code,{children:"python -c \"import whisper; whisper.load_model('base')\""})]}),"\n",(0,r.jsx)(n.li,{children:'Record yourself saying "find the red cube"'}),"\n",(0,r.jsxs)(n.li,{children:["Transcribe with: ",(0,r.jsx)(n.code,{children:"model = whisper.load_model('base'); result = model.transcribe('recording.wav'); print(result['text'])"})]}),"\n",(0,r.jsx)(n.li,{children:"Verify accuracy: Does the output match what you said?"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2713 Whisper installed and model loaded"}),"\n",(0,r.jsx)(n.li,{children:"\u2713 Audio recording captured successfully"}),"\n",(0,r.jsx)(n.li,{children:"\u2713 Transcription text matches spoken command (\u226590% accuracy)"}),"\n",(0,r.jsx)(n.li,{children:"\u2713 Confidence score provided"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-l2-2-ros-2-node-integration",children:"Exercise L2-2: ROS 2 Node Integration"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Create a ROS 2 node that publishes voice commands"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Create ",(0,r.jsx)(n.code,{children:"whisper_ros2_node.py"})," in ",(0,r.jsx)(n.code,{children:"chapter4_voice/src/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Implement ",(0,r.jsx)(n.code,{children:"listen_and_transcribe()"})," method"]}),"\n",(0,r.jsxs)(n.li,{children:["Publish to ",(0,r.jsx)(n.code,{children:"/robot/voice_command"})," topic"]}),"\n",(0,r.jsx)(n.li,{children:"Add error handling for no-speech-detected case"}),"\n",(0,r.jsxs)(n.li,{children:["Test with: ",(0,r.jsx)(n.code,{children:"ros2 run chapter4_voice whisper_ros2_node"})]}),"\n",(0,r.jsxs)(n.li,{children:["In another terminal, monitor topic: ",(0,r.jsx)(n.code,{children:"ros2 topic echo /robot/voice_command"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2713 Node launches without errors"}),"\n",(0,r.jsx)(n.li,{children:"\u2713 Microphone input captured"}),"\n",(0,r.jsx)(n.li,{children:"\u2713 Transcription published to topic"}),"\n",(0,r.jsx)(n.li,{children:"\u2713 Confidence scores logged"}),"\n",(0,r.jsx)(n.li,{children:"\u2713 Handles timeout gracefully"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"real-hardware-considerations",children:"Real Hardware Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"differences-from-simulation",children:"Differences from Simulation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simulation (Isaac Sim)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Perfect audio quality, no background noise"}),"\n",(0,r.jsx)(n.li,{children:"Microphone input simulated or perfect recording"}),"\n",(0,r.jsx)(n.li,{children:"Transcription happens instantly"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Real Hardware"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Background noise: HVAC, traffic, other people"}),"\n",(0,r.jsx)(n.li,{children:"Acoustic effects: Echo, reverberation in rooms"}),"\n",(0,r.jsx)(n.li,{children:"Microphone quality varies: USB microphone vs. built-in"}),"\n",(0,r.jsx)(n.li,{children:"Accents and speech patterns: Whisper more robust now, but not 100%"}),"\n",(0,r.jsx)(n.li,{children:"Latency: Network if using API, GPU if local"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use external USB microphone"})," (better quality than built-in)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reduce background noise"})," when possible (quiet environment)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speak clearly and at normal volume"})," (not too soft, not too loud)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use local inference"})," for real robots (API latency unacceptable for real-time)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Set confidence threshold carefully"})," (too high = missed commands, too low = false positives)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Log all transcriptions"})," for debugging and improvement"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"production-considerations",children:"Production Considerations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Voice activation"}),": Don't listen continuously (power/noise), trigger on wake word"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multiple speakers"}),": Different voices may have different confidence scores"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accents and dialects"}),": Whisper handles many, but test with your speakers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multilingual"}),": If adding non-English, models may have different accuracy"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy"}),": Local inference keeps audio data private"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Whisper is a powerful, open-source speech recognition system"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Local inference is faster and more private than cloud APIs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"ROS 2 nodes can easily integrate ML models for perception"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Confidence scores help validate transcription quality"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Error handling and edge cases are critical for reliability"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Real robots have different acoustic challenges than simulation"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.strong,{children:"Lesson 3"}),", you'll add ",(0,r.jsx)(n.strong,{children:"vision perception"})," to complement voice input:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Object detection with YOLO-v8"}),"\n",(0,r.jsx)(n.li,{children:"Semantic segmentation for scene understanding"}),"\n",(0,r.jsx)(n.li,{children:"Combining voice + vision for powerful perception"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"See you in Lesson 3! \ud83d\udc41\ufe0f\ud83c\udfa4"})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}}}]);